\documentclass[a4paper, 12pt]{article}
% packages
\usepackage{amssymb}
\usepackage[fleqn]{mathtools}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{bussproofs}
\usepackage{xcolor}
\usepackage[margin=1.3cm]{geometry}
\usepackage{logicproof}
\usepackage{diagbox}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{lstautogobble}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{tipa}
\usetikzlibrary{decorations.pathreplacing, arrows, shapes.gates.logic.US, circuits.logic.US, calc, automata, positioning, intersections}

% shorthand for verbatim
% this clashes with logicproof, so maybe fix this at some point?
\catcode`~=\active
\def~#1~{\texttt{#1}}

% code listing
\lstdefinestyle{main}{
    numberstyle=\tiny,
    breaklines=true,
    showspaces=false,
    showstringspaces=false,
    tabsize=2,
    numbers=left,
    basicstyle=\ttfamily,
    columns=fixed,
    fontadjust=true,
    basewidth=0.5em,
    autogobble,
    xleftmargin=3.0ex,
    mathescape=true
}
\newcommand{\dollar}{\mbox{\textdollar}} %
\lstset{style=main}

% augmented matrix
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
\hskip -\arraycolsep
\let\@ifnextchar\new@ifnextchar
\array{#1}}
\makeatother

% ceiling / floor
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% custom commands
\newcommand{\indefint}[2]{\int #1 \, \mathrm{d}#2}
\newcommand{\defint}[4]{\int_{#1}^{#2} #3 \, \mathrm{d}#4}
\newcommand{\pdif}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dif}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\newcommand{\limit}[2]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle{\lim_{#1 \to #2}}$}}}
\newcommand{\summation}[2]{\sum\limits_{#1}^{#2}}
\newcommand{\product}[2]{\prod\limits_{#1}^{#2}}
\newcommand{\intbracket}[3]{\left[#3\right]_{#1}^{#2}}
\newcommand{\ulsmash}[1]{\underline{\smash{#1}}}

\newcommand{\powerset}[0]{\wp}
\renewcommand{\emptyset}[0]{\varnothing}

\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{\(\m@th{#1\langle}\)}%
  \mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{\(\m@th{#1\rangle}\)}%
  \mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother
\newcommand{\lla}{\llangle}
\newcommand{\rra}{\rrangle}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\crnr}[1]{\text{\textopencorner} #1 \text{\textcorner}}
\newcommand{\laplace}{\mathcal{L}}
\newcommand{\fourier}{\mathcal{F}}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\rowt}[1]{\begin{bmatrix}
    #1
\end{bmatrix}^\top}

\newcommand{\unaryproof}[2]{\AxiomC{#1} \UnaryInfC{#2} \DisplayProof}
\newcommand{\binaryproof}[3]{\AxiomC{#1} \AxiomC{#2} \BinaryInfC{#3} \DisplayProof}
\newcommand{\trinaryproof}[4]{\AxiomC{#1} \AxiomC{#2} \AxiomC{#3} \TrinaryInfC{#4} \DisplayProof}

\newcommand{\axiom}[1]{\AxiomC{#1}}
\newcommand{\unary}[1]{\UnaryInfC{#1}}
\newcommand{\binary}[1]{\BinaryInfC{#1}}
\newcommand{\trinary}[1]{\TrinaryInfC{#1}}
\newcommand{\quaternary}[1]{\QuaternaryInfC{#1}}
\newcommand{\quinary}[1]{\QuinaryInfC{#1}}
\newcommand{\dproof}[0]{\DisplayProof}

\newcommand{\bnfsep}[0]{\ |\ }
\newcommand{\lrbt}[0]{\ \bullet\ }
\newcommand{\concsep}[0]{\ ||\ }
\newcommand{\ttbs}{\char`\\}

\newcommand{\violet}[1]{\textcolor{violet}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}

% no indent
\setlength\parindent{0pt}

% reasoning proofs
\usepackage{ltablex}
\usepackage{environ}
\keepXColumns
\NewEnviron{reasoning}{
    \begin{tabularx}{\textwidth}{rlX}
        \BODY
    \end{tabularx}
}
\newcommand{\proofline}[3]{$(#1)$ & $#2$ & \hfill #3 \smallskip \\}
\newcommand{\proofarbitrary}[1]{& take arbitrary $#1$ \smallskip \\}
\newcommand{\prooftext}[1]{\multicolumn{3}{l}{#1} \smallskip \\}
\newcommand{\proofmath}[3]{$#1$ & = $#2$ & \hfill #3 \smallskip \\}
\newcommand{\prooftherefore}[1]{& $\therefore #1$ \smallskip \\}
\newcommand{\proofbc}[0]{\prooftext{\textbf{Base Case}}}
\newcommand{\proofis}[0]{\prooftext{\textbf{Inductive Step}}}

% reasoning er diagrams
\newcommand{\nattribute}[4]{
    \node[draw, state, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\mattribute}[4]{
    \node[draw, state, accepting, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\dattribute}[4]{
    \node[draw, state, dashed, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\entity}[3]{
    \node[] (#1-c) at (#2) {#3};
    \node[inner sep=0cm] (#1-l) at ($(#1-c) + (-1, 0)$) {};
    \node[inner sep=0cm] (#1-r) at ($(#1-c) + (1, 0)$) {};
    \node[inner sep=0cm] (#1-u) at ($(#1-c) + (0, 0.5)$) {};
    \node[inner sep=0cm] (#1-d) at ($(#1-c) + (0, -0.5)$) {};
    \draw
    ($(#1-c) + (-1, 0.5)$) -- ($(#1-c) + (1, 0.5)$) -- ($(#1-c) + (1, -0.5)$) -- ($(#1-c) + (-1, -0.5)$) -- cycle;
}
\newcommand{\relationship}[3]{
    \node[] (#1-c) at (#2) {#3};
    \node[inner sep=0cm] (#1-l) at ($(#1-c) + (-1, 0)$) {};
    \node[inner sep=0cm] (#1-r) at ($(#1-c) + (1, 0)$) {};
    \node[inner sep=0cm] (#1-u) at ($(#1-c) + (0, 1)$) {};
    \node[inner sep=0cm] (#1-d) at ($(#1-c) + (0, -1)$) {};
    \draw
    ($(#1-c) + (-1, 0)$) -- ($(#1-c) + (0, 1)$) -- ($(#1-c) + (1, 0)$) -- ($(#1-c) + (0, -1)$) -- cycle;
}

% actual document
\begin{document}
    \section*{CO245 - Probability and Statistics}
        \subsection*{15th January 2020}
            Probability is a mathematical formalism used to describe and quantify uncertainty.
            \subsubsection*{Sample Spaces and Events}
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{sample space} \hfill $S$ or $\Omega$
                        \subitem a set containing the possible outcomes of a random experiment
                        \subitem for example; sample space of two coin tosses \hfill $S = \{ (H, H), (H, T), (T, H), (T, T)\}$
                    \item \textbf{event} \hfill $E$ ($E \subseteq S$)
                        \subitem any subset of the sample space (collection of some possible events)
                        \subitem for example; event of the first coin being heads in two tosses \hfill $E = \{ (H, H), (H, T) \}$
                        \subitem the extremes are $\emptyset$ (the null event) which will never occur, or $S$ (the universal event) which will always occur - there is only uncertainty when the events are strictly between the events, such that $\emptyset \subset E \subset S$
                    \item \textbf{elementary event} \hfill singleton subset containing exactly one element from $S$
                \end{itemize}
                When performing a random experiment, the outcome will be a single element $s^* \in S$.
                Then an event $E \subseteq S$ has \textbf{occurred} iff $s^* \in E$.
                If it has not occurred, then $s^* \notin E \Leftrightarrow s^* \in \bar{E}$ (can be read as not $E$).
                \medskip

                With a set of events $\{ E_1, E_2, \dots \}$, we can have the following set operations;
                \begin{itemize}
                    \itemsep0em
                    \item $\bigcup\limits_i E_i = \{ s \in S \bnfsep \exists i.\ [s \in E_i] \}$ \hfill will only occur if at least one of the events $E_i$ occurs ("or")
                    \item $\bigcap\limits_i E_i = \{ s \in S \bnfsep \forall i.\ [s \in E_i] \}$ \hfill will only occur if all of the events $E_i$ occurs ("and")
                    \item $\forall i, j.\ E_i \cap E_j = \emptyset$ \hfill ($i \neq j$) if they are mutually exclusive (at most one can occur)
                \end{itemize}
            \subsubsection*{$\sigma$-algebra}
                In an uncountably infinite set, the event set you are assigning probabilities to cannot be every subset, as the probabilities cannot be made to sum to 1 under reasonable axioms.
                \medskip

                We define the $\sigma$-algebra as the subset of events which we can assign probabilities to.
                We want to define a probability function $P$ that corresponds to the subsets of $S$ that we wish to \textbf{measure}.
                This set of subsets is referred to as $\mathfrak{S}$ (the event space), with the following three properties (corresponding to the axioms of probability);
                \begin{itemize}
                    \itemsep0em
                    \item nonempty \hfill $S \in \mathfrak{S}$
                    \item closed under complements \hfill $E \in \mathfrak{S} \Rightarrow \bar{E} \in \mathfrak{S}$
                    \item closed under countable union (therefore any countable set is fine) \hfill $E_1, E_2, \dots \in \mathfrak{S} \Rightarrow \bigcup\limits_i E_i \in \mathfrak{S}$
                \end{itemize}
                A probability measure on the pair $(S, \mathfrak{S})$ is a mapping $P : \mathfrak{S} \to [0, 1]$, satisfying the following three axioms;
                \begin{itemize}
                    \itemsep0em
                    \item $\forall E \in \mathfrak{S}.\ [0 \leq P(E) \leq 1]$
                    \item $P(S) = 1$
                    \item countably additive, for \textbf{disjoint subsets} $E_1, E_2, \dots \in \mathfrak{S}$ \hfill $P\left(\bigcup\limits_i E_i\right) = \summation{i}{} P(E_i)$
                \end{itemize}
                From these, we can derive the following;
                \begin{itemize}
                    \itemsep0em
                    \item $P(\bar{E}) = 1 - P(E)$
                        $$\underbrace{P(E) + P(E)}_\text{disjoint} = P(\underbrace{E \cup \bar{E}}_{E \cup \bar{E} = S}) = P(S) = 1$$
                    \item $P(\emptyset) = 0$ \hfill special case of the above, when $E = S$
                    \item for any events $E$ and $F$ \hfill $P(E \cup F) = P(E) + P(F) - P(E \cap F)$
                \end{itemize}
        \subsection*{16th January 2020}
            \subsubsection*{Independent Events}
                It's important to note that independent events are \textbf{not} the same as disjoint events.
                Two events $E$ and $F$ are independent iff $P(E \cap F) = P(E) P(F)$ - sometimes written as $E \perp F$.
                Generally, a set of events $\{ E_1, E_2, \dots \}$ are set to be independent if for any finite subset $\{ E_{i_1}, E_{i_2}, \dots, E_{i_n} \}$;
                $$P\left(\bigcap\limits_{j = 1}^n E_{i_j}\right) = \product{j = 1}{n}P(E_{i_j})$$
                Where we have $\{ i_j \bnfsep 1 \leq j \leq n \}$ is any set of distinct positive integers.
                Note that independence is more than just pairwise independence.
                \medskip

                We propose that if events $E$ and $F$ are independent, then $\bar{E}$ and $F$ are also independent.
                Note that $E$ and $\bar{E}$ form a partition of $S$ (they are disjoint, and union to $S$).
                $F = (E \cap F) \cup (\bar{E} \cap F)$ is a disjoint union (and also a partition of $F$), this gives us $P(F) = P(E \cap F) + P(\bar{E} \cap F) \Rightarrow P(\bar{E} \cap F) = P(F) - P(E \cap F)$;
                \begin{align*}
                    P(\bar{E} \cap F) & = P(F) - P(E \cap F) & \text{$E$ and $F$ are independent}, \Rightarrow \\
                    & = P(F) - P(E) P(F) & \Rightarrow \\
                    & = (1 - P(E))P(F) & \text{probability of complement}, \Rightarrow \\
                    & = P(\bar{E}) P(F) & \text{hence independent}, \blacksquare
                \end{align*}
            \subsubsection*{Interpretations of Probability}
                In order to assign meaning to $P$, we need to have some interpretation of probability, such as the following;
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{classical}
                        \medskip

                        If $S$ is finite, and the elementary events are "equally likely", then for an event $E \subseteq S$, the probability is the number of outcomes in $E$ out of the total number of possible outcomes ($S$);
                        $$P(E) = \frac{| E |}{| S |}$$
                        This idea of "equally likely" (uniform) can be extended to infinite spaces.
                        Instead of taking the set cardinality, another standard measure (such as area or volume) can be used instead.
                    \item \textbf{frequentist}
                        \medskip

                        The idea is that if someone were to perform the same experiment ($E$ may or may not occur) in identical random situations many times, then the proportion of times $E$ occurs will tend to some limiting value, which would be $P(E)$.
                    \item \textbf{subjective}
                        \medskip

                        Not assessed.
                        Probability is the degree of belief held by an individual (see \textit{De Finetti}) - suppose a random event $E \subseteq S$ is to be performed, and an individual enters a game regarding this experiment, with two choices;
                        \begin{itemize}
                            \itemsep0em
                            \item gamble \hfill if $E$ occurs they win \$1, otherwise if $\bar{E}$ occurs they win \$0
                            \item stick \hfill regardless of the outcome, the individual receives \$$P(E)$
                        \end{itemize}
                        The critical value of $P(E)$, where the individual is indifferent between the choices, is their probability of $E$.
                \end{itemize}
            \subsubsection*{Dependent Probabilities and Conditional Probability}
                For the standard example of flipping a coin and rolling a die (assuming both fair), we have independence - the probability of each elementary event is $\frac{1}{2} \cdot \frac{1}{6} = \frac{1}{12}$.
                \medskip

                However, consider the case where we have two die, where the first is fair, and the second is a "top", where we only have odd numbers (such that a roll of a 2 is mapped to a 5, 4 to 3, and 6 to 1).
                When we now flip the coin, if it is heads, we use the normal die, otherwise if it is tails, we use the "top".
                As expected, this is no longer independent.
                \medskip

                For two events $E$ and $F$ in $S$, where $P(F) \neq 0$, we can define the probability of $E$ occurring, given that we know $F$ has occurred to be;
                $$P(E|F) = \frac{P(E \cap F)}{P(F)}$$
                Note that this also holds for independence ($P(E)$ doesn't change, as expected);
                $$P(E|F) = \frac{P(E \cap F)}{P(F)} = \frac{P(E) P(F)}{P(F)} = P(E)$$
                An example of this is as follows - suppose we roll two normal dice, with one from each hand.
                The sample space is all the ordered pairs of possible values $S = \{ (1, 1), (1, 2), \dots, (6, 6) \}$.
                Let the event $E$ be defined as the die from the left hand has a higher value than the die from the right hand.
                Looking at all possible combinations, we have;
                $$P(E) = \frac{15}{36}$$
                Suppose we now know $F$, the value of the left die being 5, has occurred.
                Since we know $F$ has occurred, the only events that could have happened are $F = \{ (5, 1), (5, 2), \dots, (5, 6) \}$.
                Similarly, the only sample space elements in $E$ that could've occurred are $E \cap F = \{ (5, 1), (5, 2), (5, 3), (5, 4) \}$.
                Our probability is as follows;
                $$\frac{| E \cap F |}{| F |} = \frac{4}{6} = \frac{\frac{4}{36}}{\frac{1}{6}} = \frac{P(E \cap F)}{P(F)} \equiv P(E|F)$$

                One way to think about probability conditioning as a shrinking of the sample space, with events being replaced by intersections with the reduced space, and a rescaling of the probabilities.
                For example, with $F = S$, we have the following;
                $$P(E) = \frac{P(E)}{1} = \frac{P(E \cap S)}{P(S)} = P(E|S)$$
                Furthermore, we can extend the idea of independence of events with respect to a probability measure $P$ to conditional probabilities.
                $P(\cdot |F)$ is a valid probability measure which obeys the axioms of probability on the set $F$.
                For three events $E_1, E_2, F$, the event pair $E_1$ and $E_2$ are conditionally independent given $F$ (sometimes written as $E_1 \bot E_2|F$) if and only if;
                \begin{center}
                    $P(E_1 \cap E_2 | F) = P(E_1|F) P(E_2|F)$
                \end{center}
            \subsubsection*{Bayes Theorem}
                For two events $E$ and $F$ in $S$, we have $P(E \cap F) = P(F) P(E|F)$, and $P(E \cap F) = P(E) P(F|E)$ (interchanging, and noting commutativity of $\cap$).
                Hence we have Bayes Theorem;
                $$P(E|F) = \frac{P(E) P(F|E)}{P(F)}$$
            \subsubsection*{Partition Rule}
                Consider a set of events $\{ F_1, F_2, \dots \}$, which form a partition of $S$ (they are disjoint, and union together to form $S$).
                Then for any event $E \subseteq S$, the partition rule states;
                $$P(E) = \summation{i}{} P(E|F_i) P(F_i)$$
                The proof is as follows;
                \begin{align*}
                    E & = E \cap S \\
                    & = E \cap \bigcup\limits_i F_i & \text{by definition of partitions} \\
                    & = \bigcup\limits_i (E \cap F_i) & \text{by distributivity of intersection} \\
                    P(E) & = P\left(\bigcup\limits_i (E \cap F_i)\right) \\
                    & = \summation{i}{} P(E \cap F_i) & \text{disjoint union} \\
                    & = \summation{i}{} P(E|F_i) P(F_i)
                \end{align*}
                Note that $\{ E \cap F_1, E \cap F_2, \dots \}$ is disjoint if $\{ F_1, F_2, \dots \}$ is.
                Assume there is an element $s \in E \cap F_i$ and $s \in E \cap F_j$ (where $i \neq j$), if it is in both, then $s \in F_i$ and $s \in F_j$, which is not possible.
                \medskip

                Note that $\{ F, \bar{F} \}$ forms a partition of $S$, therefore by the Law of Total Probability we have;
                \begin{center}
                    $P(E) = P(E \cap F) + P(E \cap \bar{F}) = P(E|F) P(F) + P(E|\bar{F}) P(\bar{F})$
                \end{center}
            \subsubsection*{Terminology}
                \begin{itemize}
                    \itemsep0em
                    \item conditional probabilities \hfill $P(E|F)$
                    \item joint probabilities \hfill $P(E \cap F)$
                    \item marginal probabilities (margins of a table) \hfill $P(E)$
                        \subitem margins of a table
                \end{itemize}
            \subsubsection*{Likelihood and Posterior Probability}
                Suppose we have a probability model with parameters $\theta$, that define a model instance (such as $\mu$ and $\sigma$), and a set of observations (or evidence) $X$.
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{likelihood function} (probability of the evidence, given  the parameters) \hfill $P(X|\theta)$
                        \subitem what is the probability our model will predict that evidence?
                    \item \textbf{posterior probability} (probability of the parameters, given the evidence) \hfill $P(\theta|X)$
                        \subitem what is the probability the actual parameters are $\theta$, given our evidence?
                    \item \textbf{prior probability} (not taking into account the evidence) \hfill $P(\theta)$
                \end{itemize}
                This is related by Bayes theorem;
                $$P(\theta|X) = \frac{P(X|\theta) P(\theta)}{P(X)}$$
                \begin{center}
                    posterior probability $\propto$ likelihood $\times$ prior probability
                \end{center}
                This is then divided by the normalising constant;
                $$\summation{\theta}{} P(X|\theta) P(\theta) = P(X)$$
\end{document}