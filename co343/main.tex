\documentclass[a4paper, 12pt]{article}

% packages
\usepackage{amssymb}
\usepackage[fleqn]{mathtools}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{bussproofs}
\usepackage{xcolor}
\usepackage[margin=1.3cm]{geometry}
\usepackage{logicproof}
\usepackage{diagbox}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{lstautogobble}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{tipa}
\usepackage{pgfplots}
\usepackage{adjustbox}

% tikz libraries
\usetikzlibrary{
    decorations.pathreplacing,
    arrows,
    shapes,
    shapes.gates.logic.US,
    circuits.logic.US,
    calc,
    automata,
    positioning,
    intersections
}

\pgfplotsset{compat=1.16}

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\allowdisplaybreaks % allow environments to break
\setlength\parindent{0pt} % no indent

% shorthand for verbatim
% this clashes with logicproof, so maybe fix this at some point?
\catcode`~=\active
\def~#1~{\texttt{#1}}

% code listing
\lstdefinestyle{main}{
    numberstyle=\tiny,
    breaklines=true,
    showspaces=false,
    showstringspaces=false,
    tabsize=2,
    numbers=left,
    basicstyle=\ttfamily,
    columns=fixed,
    fontadjust=true,
    basewidth=0.5em,
    autogobble,
    xleftmargin=3.0ex,
    mathescape=true
}
\newcommand{\dollar}{\mbox{\textdollar}} %
\lstset{style=main}

% augmented matrix
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
\hskip -\arraycolsep
\let\@ifnextchar\new@ifnextchar
\array{#1}}
\makeatother

% ceiling / floor
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% custom commands
\newcommand{\indefint}[2]{\int #1 \, \mathrm{d}#2}
\newcommand{\defint}[4]{\int_{#1}^{#2} #3 \, \mathrm{d}#4}
\newcommand{\pdif}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dif}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\newcommand{\limit}[2]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle{\lim_{#1 \to #2}}$}}}
\newcommand{\limitsup}[2]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle{\limsup_{#1 \to #2}}$}}}
\newcommand{\summation}[2]{\sum\limits_{#1}^{#2}}
\newcommand{\product}[2]{\prod\limits_{#1}^{#2}}
\newcommand{\intbracket}[3]{\left[#3\right]_{#1}^{#2}}
\newcommand{\laplace}{\mathcal{L}}
\newcommand{\fourier}{\mathcal{F}}
\newcommand{\mat}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\rowt}[1]{\begin{bmatrix}
    #1
\end{bmatrix}^\top}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\lto}[0]{\leadsto\ }

\newcommand{\ulsmash}[1]{\underline{\smash{#1}}}

\newcommand{\powerset}[0]{\wp}
\renewcommand{\emptyset}[0]{\varnothing}

\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{\(\m@th{#1\langle}\)}%
  \mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{\(\m@th{#1\rangle}\)}%
  \mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother
\newcommand{\lla}{\llangle}
\newcommand{\rra}{\rrangle}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\crnr}[1]{\text{\textopencorner} #1 \text{\textcorner}}
\newcommand{\bnfsep}[0]{\ |\ }
\newcommand{\concsep}[0]{\ ||\ }

\newcommand{\axiom}[1]{\AxiomC{#1}}
\newcommand{\unary}[1]{\UnaryInfC{#1}}
\newcommand{\binary}[1]{\BinaryInfC{#1}}
\newcommand{\trinary}[1]{\TrinaryInfC{#1}}
\newcommand{\quaternary}[1]{\QuaternaryInfC{#1}}
\newcommand{\quinary}[1]{\QuinaryInfC{#1}}
\newcommand{\dproof}[0]{\DisplayProof}
\newcommand{\llabel}[1]{\LeftLabel{\scriptsize #1}}
\newcommand{\rlabel}[1]{\RightLabel{\scriptsize #1}}

\newcommand{\ttbs}{\char`\\}
\newcommand{\lrbt}[0]{\ \bullet\ }

% colours
\newcommand{\violet}[1]{\textcolor{violet}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\teal}[1]{\textcolor{teal}{#1}}

% reasoning proofs
\usepackage{ltablex}
\usepackage{environ}
\keepXColumns
\NewEnviron{reasoning}{
    \begin{tabularx}{\textwidth}{rlX}
        \BODY
    \end{tabularx}
}
\newcommand{\proofline}[3]{$(#1)$ & $#2$ & \hfill #3 \smallskip \\}
\newcommand{\proofarbitrary}[1]{& take arbitrary $#1$ \smallskip \\}
\newcommand{\prooftext}[1]{\multicolumn{3}{l}{#1} \smallskip \\}
\newcommand{\proofmath}[3]{$#1$ & = $#2$ & \hfill #3 \smallskip \\}
\newcommand{\prooftherefore}[1]{& $\therefore #1$ \smallskip \\}
\newcommand{\proofbc}[0]{\prooftext{\textbf{Base Case}}}
\newcommand{\proofis}[0]{\prooftext{\textbf{Inductive Step}}}

% ER diagrams
\newcommand{\nattribute}[4]{
    \node[draw, state, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\mattribute}[4]{
    \node[draw, state, accepting, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\dattribute}[4]{
    \node[draw, state, dashed, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\entity}[3]{
    \node[] (#1-c) at (#2) {#3};
    \node[inner sep=0cm] (#1-l) at ($(#1-c) + (-1, 0)$) {};
    \node[inner sep=0cm] (#1-r) at ($(#1-c) + (1, 0)$) {};
    \node[inner sep=0cm] (#1-u) at ($(#1-c) + (0, 0.5)$) {};
    \node[inner sep=0cm] (#1-d) at ($(#1-c) + (0, -0.5)$) {};
    \draw
    ($(#1-c) + (-1, 0.5)$) -- ($(#1-c) + (1, 0.5)$) -- ($(#1-c) + (1, -0.5)$) -- ($(#1-c) + (-1, -0.5)$) -- cycle;
}
\newcommand{\relationship}[3]{
    \node[] (#1-c) at (#2) {#3};
    \node[inner sep=0cm] (#1-l) at ($(#1-c) + (-1, 0)$) {};
    \node[inner sep=0cm] (#1-r) at ($(#1-c) + (1, 0)$) {};
    \node[inner sep=0cm] (#1-u) at ($(#1-c) + (0, 1)$) {};
    \node[inner sep=0cm] (#1-d) at ($(#1-c) + (0, -1)$) {};
    \draw
    ($(#1-c) + (-1, 0)$) -- ($(#1-c) + (0, 1)$) -- ($(#1-c) + (1, 0)$) -- ($(#1-c) + (0, -1)$) -- cycle;
}

% AVL Trees
\newcommand{\avltri}[4]{
    \draw ($(#1)$) -- ($(#1) + #4*(0.5, -1)$) -- ($(#1) + #4*(-0.5, -1)$) -- cycle;
    \node at ($(#1) + #4*(0, -1) + (0, 0.5)$) {#3};
    \node at ($(#1) + #4*(0, -1) + (0, -0.5)$) {#2};
}

% RB Trees
\tikzset{rbtr/.style={inner sep=2pt, circle, draw=black, fill=red}}
\tikzset{rbtb/.style={inner sep=2pt, circle, draw=black, fill=black}}

% Samples
\tikzset{spos/.style={inner sep=2pt, circle, draw=black, fill=blue!20}}
\tikzset{sneg/.style={inner sep=2pt, circle, draw=black, fill=red!20}}

% Joins
\newcommand\ljoin{\stackrel{\mathclap{\normalfont\mbox{\tiny L}}}{\bowtie}}
\newcommand\rjoin{\stackrel{\mathclap{\normalfont\mbox{\tiny R}}}{\bowtie}}
\newcommand\ojoin{\stackrel{\mathclap{\normalfont\mbox{\tiny O}}}{\bowtie}}

\setcounter{MaxMatrixCols}{100}

% actual document
\begin{document}
    {\sc Computing $3^\text{rd}$ Year Notes} \hfill ~https://github.com/lin-e/imperial-revision~
    \rule{\textwidth}{0.1pt}
    \section*{CO343 - Operations Research \hfill (60016)}
        \subsection*{Lecture 1}
            Operations research is the science of taking decisions, it's a branch of applied mathematics where we attempt to model problems where need to make a decision.
            The decisions aren't arbitrary, and we want to attempt to score each decision based on some metric (such as time, cost, etc.), to find the optimal solution.
            \medskip

            The course focuses on formulating a mathematical model to represent the problem, and then developing a computer-based procedure for deriving solutions to the problem from the model.
            Assume our goal was the following;
            $$\min_{\vec{x}} z = f(\vec{x}) \hspace{50pt} \text{subject to } \vec{x} \in \mathcal{X}$$
            \begin{itemize}
                \itemsep0em
                \item \textbf{decision variables} \hfill $\vec{x} \in \mathbb{R}^n$
                \item \textbf{objective function} \hfill $f : \mathbb{R}^n \to \mathbb{R}$
                \item \textbf{feasible set} (set of admissible decisions) \hfill $\mathcal{X} \subseteq \mathbb{R}^n$
                \item \textbf{optimal solution} (any vector that minimises $f$) \hfill $\vec{x^*}$
                \item \textbf{optimal value} \hfill $z^* = f(\vec{x^*})$
            \end{itemize}
            \subsubsection*{Linear Programming}
                A linear program optimises a \textbf{linear objective function}, where a feasible set is described by linear equality / inequality constraints.
                Compared to non-linear problems, where a \textbf{local} maximum may vary (and therefore be sub-optimal) depending on the starting search position, this isn't a concern for linear problems.
                \medskip

                We can say the polygon representing a two dimensional feasible set is convex if the points on the line joining two points in the feasible set are also in the polygon.
                If this region is convex and linear, it can be proven that a local optimum is also a global optimum.
                For example, take $x$ and $x^\prime$;
                \begin{center}
                    \begin{tikzpicture}[x=0.75cm, y=0.75cm]
                        \draw[fill=red!10]
                        (0, 0) -- (5, 1) -- (4, -3) -- (0, -4) -- (-1, -2) -- cycle;

                        \node (x) at (0, -3) {$x$};
                        \node (xp) at (4.5, 0.5) {$x^\prime$};

                        \draw (x) edge[dashed] (xp);
                    \end{tikzpicture}
                \end{center}
            \subsubsection*{Linear Programming Example}
                A manufacturer produces $A$ (acid) and $C$ (caustic) and wants to decide a production plan.
                The ingredients for $A$ and $C$ are $X$ (a sulphate) and $Y$ (sodium).
                \begin{itemize}
                    \itemsep0em
                    \item each ton of $A$ requires 2 tons of $X$ and 1 ton of $Y$
                    \item each ton of $C$ requires 1 ton of $X$ and 3 tons of $Y$
                    \item supply of $X$ is limited to 11 tons per week
                    \item supply of $Y$ is limited to 18 tons per week
                    \item $A$ sells for £1000 per ton
                    \item $C$ sells for £1000 per ton
                    \item a maximum of 4 tons of $A$ can be sold per week
                \end{itemize}
                Our goal is to maximise weekly value of sales of $A$ and $C$.
                To determine how much $A$ and $C$ to produce, we need to formulate a \textbf{mathematical programming model};
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{decision variables}
                        \begin{itemize}
                            \itemsep0em
                            \item weekly production of $A$ (tons) \hfill $x_1$
                            \item weekly production of $B$ (tons) \hfill $x_2$
                        \end{itemize}
                    \item \textbf{objective function} (weekly profit in £1000s) \hfill $z = f(x_1, x_2)$
                    \item \textbf{feasible set} \hfill $\vec{x} = (x_1, x_2) \in \mathcal{X}$
                \end{itemize}
                A \textbf{production plan} is representable as $\vec{x} = (x_1, x_2)$.
                The objective function can be written as $z = x_1 + x_2$.
                Another constraint is that \violet{$x_1 \geq 0$} and \violet{$x_2 \geq 0$}; we cannot produce a negative amount of a product.
                $x_1$ tons of $A$ and $x_2$ tons of $C$ requires $2x_1 + x_2$ tons of $X$, and we know that is limited to 11 tons per week; therefore we have the constraint \blue{$2x_1 + x_2 \leq 11$}.
                Similarly, we also have the limitation of \red{$x_1 + 3x_2 \leq 18$}, because of the limitations of $Y$.
                Finally, we have another restriction that we cannot sell more than 4 tons of $A$, therefore \teal{$x_1 \leq 4$}.
                \medskip

                To get the overall feasible set, we intersect the feasible set of all the constraints to get the following;
                \begin{center}
                    \begin{tikzpicture}[x=0.5cm, y=0.5cm]
                        \draw
                        (0, -1) edge[->, left] node{$x_2$} (0, 15)
                        (-1, 0) edge[->, below] node{$x_1$} (20, 0);

                        \draw
                        (14, 0) edge[dashed] (0, 14)
                        (6.5, 6.5) edge[<->] (7.5, 7.5);

                        \draw[fill=black!20] (0, 0) -- (0, 6) -- (3, 5) -- (4, 3) -- (4, 0) -- cycle;

                        \draw
                        (0, 0) edge[violet, line width=2pt] (0, 14)
                        (0, 0) edge[violet, line width=2pt] (19, 0)
                        (0, 11) edge[blue, line width=2pt] (5.5, 0)
                        (0, 6) edge[red, line width=2pt] (18, 0)
                        (4, 0) edge[teal, line width=2pt] (4, 14);
                    \end{tikzpicture}
                \end{center}
                Each of the following vertices is the intersection of constraints, which can be obtained by solving the linear equation of each line;
                \begin{align*}
                    O & = (0, 0) \\
                    P & = (0, 6) \\
                    Q & = (3, 5) \\
                    R & = (4, 3) \\
                    S & = (4, 0)
                \end{align*}
                By moving the objective function (the dashed line), in the direction of the arrows, we can see that the $z$ value increases further away from the origin, and therefore the graphical result that results in the highest value is $Q$.
                Typically the optimal solution lies on a vertex, however in some cases, there can be multiple solutions (an edge when the objective function is parallel to the constraint, or all the points in the feasible set in the case of a constant objective function).
                \medskip

                The simplest algorithm is to enumerate all the vertices (intersections) of the feasible set, however this can have exponential complexity in the worst case and the number of vertices grow quite quickly in higher dimensions.
                The \textbf{Simplex Algorithm} finds an optimal vertex, often inspecting a \textbf{small subset} of the total.
                \medskip

                We can vary this example, for example if we wanted to minimise $z = 3x_1 - x_2$ over the feasible set, we can examine the objective function at each of the vertices;
                \begin{center}
                    \begin{tabular}{c|c|c|c|c}
                        $O = (0, 0)$ & $P = (0, 6)$ & $Q = (3, 5)$ & $R = (4, 3)$ & $S = (4, 0)$ \\
                        \hline
                        0 & -6 & 4 & 9 & 12
                    \end{tabular}
                \end{center}
                This therefore gives us $P = (x_1, x_2) = (0, 6)$ as the optimal.
                \medskip

                On the other hand, if we were to maximise $z = 2x_1 + x_2$, any point on the line segment $QR$ would be optimal; this tells us that points other than the vertices can be optimal, but there is at least one optimal vertex.
                \medskip

                Additionally, if we were to set a production goal of 7 tons of $A$, we'd have an empty feasible set, since $x_1 \geq 7$ would cause an empty set with $x_1 \leq 4$.
                In this case, the LP is \textbf{infeasible}.
                Similarly, if the constraints on $X$ and $Y$ were removed, the objective function could grow to $+\infty$, hence the LP is \textbf{unbounded}.
        \subsection*{Lecture 2}
            \subsubsection*{Standard Form}
                In order to use a computer to solve an LP problem, we need to define a \textbf{standard form};
                \begin{itemize}
                    \itemsep0em
                    \item the goal is to \textbf{minimise} a \textbf{linear} objective function
                    \item all constraints are linear equality constraints
                    \item all constraint right hand sides are non-negative
                    \item all decision variables are non-negative
                \end{itemize}
                A linear problem in standard form is as follows;
                $$\begin{matrix}
                    \text{minimise} & z = c_1 x_1 & + & c_2 x_2 & + & \cdots & c_n x_n \\ \\
                    \text{subject to} & a_{1, 1} x_1 & + & a_{1, 2} x_2 & + & \cdots & a_{1, n} x_n & = & b_1 \\
                    & a_{2, 1} x_1 & + & a_{2, 2} x_2 & + & \cdots & a_{2, n} x_n & = & b_2 \\
                    & \vdots & & \vdots & & & \vdots & & \vdots \\
                    & a_{m, 1} x_1 & + & a_{m, 2} x_2 & + & \cdots & a_{m, n} x_n & = & b_m
                \end{matrix}$$
                This has the constraints that all decision variables $\forall i \in [1, n]\ x_i \geq 0$ and $\forall i \in [1, m]\ b_i \geq 0$.
                The \textbf{input parameters} $b_i$, $c_j$, and $a_{i, j}$ are fixed real constants.
                Clearly, this can be written more compactly as the following;
                \begin{align*}
                    \mat{A} & = \begin{bmatrix}
                        a_{1, 1} & a_{1, 2} & \cdots & a_{1, n} \\
                        a_{2, 1} & a_{2, 2} & \cdots & a_{2, n} \\
                        \vdots & \vdots & \ddots & \vdots \\
                        a_{m ,1} & a_{m, 2} & \cdots & a_{m, n}
                    \end{bmatrix} \\
                    \vec{b} & = \begin{bmatrix}
                        b_1 \\ b_2 \\ \vdots \\ b_m
                    \end{bmatrix} \\
                    \vec{x} & = \begin{bmatrix}
                        x_1 \\ x_2 \\ \vdots \\ x_n
                    \end{bmatrix} \\
                    \vec{c} & = \begin{bmatrix}
                        c_1 \\ c_2 \\ \vdots \\ c_n
                    \end{bmatrix}
                \end{align*}
                Therefore, the equation can be written as;
                $$\text{minimise } \vec{z} = \vec{c}^\top \vec{x} \text{ subject to } \mat{A}\vec{x} = \vec{b}$$
                Note that $\vec{x} \geq 0$ and $\vec{b} \geq 0$, which means that it holds \textbf{component-wise} (such that $\forall x_i \in \vec{x}\ x_i \geq 0$).
            \subsubsection*{Standardising}
                This follows the example in tutorial 1.
                \medskip

                Our goal is to maximise $y = 2x_1 + x_2$, (s.t.) subject to;
                \begin{itemize}
                    \itemsep0em
                    \item $x_1 - 4x_2 \leq 1$
                    \item $-x_1 - 5x_2 \leq -3$
                    \item $x_1, x_2 \geq 0$
                \end{itemize}
                We can do the following conversion steps to get the equations into the standard form.
                To reformulate inequalities as equalities, we introduced the \textbf{slack variables} $s_1$ and $s_2$.
                All that is left to do is to convert the maximisation into a minimisation, which can be done by negating the objective function.
                \begin{align*}
                    x_1 - 4x_2 & \leq 1 & \Rightarrow \\
                    x_1 - 4x_2 + \violet{s_1} & = 1 \\
                    -x_1 - 5x_2 & \leq 3 & \Rightarrow \\
                    x_1 + 5x_2 & \geq -3 & \Rightarrow \\
                    x_1 + 5x_2 - \blue{s_2} & = -3 \\
                    x_1, x_2, \violet{s_1}, \blue{s_2} & \geq 0 \\
                    \text{(maximise) } y & = 2x_1 + x_2 & \Rightarrow \\
                    \text{(minimise) } z & = -2x_1 - x_2
                \end{align*}
                Therefore, we can therefore say a minimisation of $\vec{z} = \vec{c}^\top\vec{x}$ subject to $\mat{A}\vec{x} \leq \vec{b}$ and $\vec{x} \geq 0$ is equivalent to the same minimisation subject to $\mat{A}\vec{x} + \vec{s} = \vec{b}$ and $\vec{x}, \vec{s} \geq 0$.
                The slack variables take the value of the difference $\vec{b} - \mat{A}\vec{x}$.
                Similarly, \textbf{excess variables} are the same, but instead of being added to the left hand side of the inequality, they are subtracted, and therefore take the value of the difference $\mat{A}\vec{x} - \vec{b}$.
                Additionally, a change of sign for the right hand side is trivial, as it can be done by multiplying the entire inequality by $-1$.
            \subsubsection*{Free Variables}
                Suppose the constraint $x_j \geq 0$ does not exist, such that it can be positive or negative.
                We can do this by substituting $x_j = x_j^+ - x_j^-$.
                The LP now has the following $n + 1$ variables;
                $$x_1, \dots, x_{j - 1}, x_j^+, x_j^-, x_{j + 1}, \dots, x_n$$
                Another approach to introduce free variables is to use substitution.
                Any \textbf{equality constraint} involving $x_j$ can be used to eliminate $x_j$, as for $x_1$ in the following conditions (with the substitution of $x_1 = 5 - 3x_2 - x_3$);
                \begin{align*}
                    \text{(minimise) } z & = x_1 + 3x_2 + 4x_3 \\
                    x_1 + 2x_2 + x_3 & = 5 \\
                    2x_1 + 3x_2 + x_3 & = 6 & \Rightarrow \\ \\
                    \violet{\text{(minimise) } z} &\ \violet{= x_2 + 3x_3 + 5} \\
                    \violet{x_2 + x_3} &\ \violet{= 4}
                \end{align*}
            \subsubsection*{Tutorial}
                \begin{enumerate}[1.]
                    \itemsep0em
                    \setcounter{enumi}{1}
                    \item
                        A company produces laptops at two factories, $A$ and $B$.
                        In factory $A$, $s_A$ laptops are produced a year, and $s_B$ laptops are produced a year in factory $B$.
                        The three stores, 1, 2, and 3, sell $d_1$, $d_2$, and $d_3$ a year.
                        The cost of shipping a laptop from the factory $i \in \{A, B\}$ to store $j \in \{1, 2, 3,\}$ is $c_{i, j}$.
                        Assume that the demand of all stores can be satisfied, such that $s_A + s_B \geq d_1 + d_2 + d_3$.
                        \begin{enumerate}[1.]
                            \itemsep0em
                            \item How should the laptops be shipped from the two factories to minimise shipping costs, assuming the following;
                                \begin{align*}
                                    \begin{bmatrix}
                                        s_A \\ s_B
                                    \end{bmatrix} & = \begin{bmatrix}
                                        3 \\ 3
                                    \end{bmatrix} \\
                                    \begin{bmatrix}
                                        d_1 \\ d_2 \\ d_3
                                    \end{bmatrix} & = \begin{bmatrix}
                                        2 \\ 2 \\ 2
                                    \end{bmatrix} \\
                                    (c_{i, j}) & = \begin{bmatrix}
                                        1 & 2 & 1 \\
                                        2 & 1 & 2
                                    \end{bmatrix} & \text{(first row corresponds to store $A$)}
                                \end{align*}
                            \item Formulate the optimisation model corresponding to the previous question, using the general parameters;
                                \medskip

                                Note that we will denote the number of laptops from each factory $i \in \{A, B\}$ to store $j \in \{1, 2, 3\}$ as $x_{i, j}$.
                                We therefore want to minimise the following;
                                $$z = \summation{i}{} \summation{j}{} c_{i, j}x_{i, j}$$
                                Under the following conditions;
                                \begin{align*}
                                    x_{A, j} + x_{B, j} & = d_j & \forall j \in \{1, 2, 3\} \\
                                    x_{i, 1} + x_{i, 2} + x_{i, 3} & \leq s_i & \forall i \in \{A, B\} \\
                                    x_{i, j} & \geq 0 & \forall i, \forall j
                                \end{align*}
                                It's important to note that satisfying demand is to use equality, as we can reduce the amount of computation we need to do.
                        \end{enumerate}
                \end{enumerate}
        \subsection*{Lecture 3}
            We now only focus on LPs in \textbf{standard form};
            minimise $z = \vec{c}^\top\vec{x}$, subject to $\mat{A}\vec{x} = \vec{b}$ and $\vec{x} \geq 0$, where $\mat{A} \in \mathbb{R}^{m \times n}, \vec{b} \in \mathbb{R}^m \geq 0, c \in \mathbb{R}^n$.
            We also assume that (the number of variables) $n \geq m$ (the number of equations), otherwise the system $\mat{A}\vec{x} = \vec{b}$ is overdetermined.
            Similarly, we also assume that the rows of $\mat{A}$ are linearly independent, otherwise constraints are redundant or consistent.
            Therefore, we can say $\text{rk}(\mat{A}) = m$.
            If there is linear dependence, we have either;
            \begin{itemize}
                \itemsep0em
                \item \textbf{contradictory constraints} (no solution)
                    \begin{align*}
                        x_1 + x_2 & = 1 \\
                        x_1 + x_2 & = 2
                    \end{align*}
                \item \textbf{redundant constraints}
                    \begin{align*}
                        x_1 + x_2 & = 1 \\
                        2x_1 + 2x_2 & = 2
                    \end{align*}
            \end{itemize}
            For now, we focus only on the system of linear equations in $\mathcal{LP}$;
            $$\mat{A}\vec{x} = \vec{b}$$
            Let $\mat{A} = [\vec{a_1}, \dots, \vec{a_n}]$ where $a_i \in \mathbb{R}^m$ is the $i^\text{th}$ column vector of $\mat{A}$.
            We want to select a subset of $m$ columns $\vec{a_i}$ that are linearly independent - which will always be possible since $n \geq m = \text{rk}(\mat{A})$.
            This gives us a square matrix for us to solve.
            The \textbf{index set} $I$ consists of the indices for those $m$ columns, hence $I \subseteq \{1, \dots, n\}$.
            We define the matrix $\mat{B} = B(I) \in \mathbb{R}^{m \times m}$ consisting of the columns $\{\vec{a_i}\}_{i \in I}$ as the \textbf{basis} corresponding to the index set $I$.
            \medskip

            We define a solution $\vec{x}$ to $\mat{A}\vec{x} = \vec{b}$ with $\forall i \notin I\ (x_i = 0)$ as a \textbf{basic solution (BS)} to $\mat{A}\vec{x} = \vec{b}$ with respect to the index set $I$.
            Similarly, we define a solution $\vec{x}$ satisfying both $\mat{A}\vec{x} = \vec{b}$ and $\vec{x} \geq 0$ as a \textbf{feasible solution (FS)}.
            A feasible solution, which is also basic, is a \textbf{basic feasible solution (BFS)}.
            \medskip

            Assume, for the example $I = \{1, \dots, m\}$.
            $$\begin{matrix}
                a_{1, 1}x_1 & + & \dots & + & a_{1, m}x_m & + & a_{1, m + 1}x_{m + 1} & + & \dots & + & a_{1, n}x_n & = & b_1 \\
                a_{2, 1}x_1 & + & \dots & + & a_{2, m}x_m & + & a_{2, m + 1}x_{m + 1} & + & \dots & + & a_{2, n}x_n & = & b_2 \\
                \vdots & & & & \vdots & & \vdots & & & & \vdots & & \vdots \\
                a_{m, 1}x_1 & + & \dots & + & a_{m, m}x_m & + & a_{m, m + 1}x_{m + 1} & + & \dots & + & a_{m, n}x_n & = & b_m
            \end{matrix}$$
            This is then equivalent to $\mat{B}\vec{x_B} = \vec{b}$;
            $$\begin{matrix}
                a_{1, 1}x_1 & + & \dots & + & a_{1, m}x_m & + & a_{1, m + 1}0 & + & \dots & + & a_{1, n}0 & = & b_1 \\
                a_{2, 1}x_1 & + & \dots & + & a_{2, m}x_m & + & a_{2, m + 1}0 & + & \dots & + & a_{2, n}0 & = & b_2 \\
                \vdots & & & & \vdots & & \vdots & & & & \vdots & & \vdots \\
                a_{m, 1}x_1 & + & \dots & + & a_{m, m}x_m & + & a_{m, m + 1}0 & + & \dots & + & a_{m, n}0 & = & b_m
            \end{matrix}$$
            By removing the 0 terms, we can simplify it to the following;
            $$\begin{matrix}
                a_{1, 1}x_1 & + & \dots & + & a_{1, m}x_m & = & b_1 \\
                a_{2, 1}x_1 & + & \dots & + & a_{2, m}x_m & = & b_2 \\
                \vdots & & & & \vdots & & \vdots \\
                a_{m, 1}x_1 & + & \dots & + & a_{m, m}x_m & = & b_m
            \end{matrix}$$
            We can observe that the \textbf{basic solution} corresponding to $I$ is unique, since the vectors $\{\vec{a_i}\}_{i \in I}$ are linearly independent, the basis $\mat{B}$ is invertible, and has the following unique solution;
            $$\vec{x_B} = \mat{B}^{-1}\vec{b} \in \mathbb{R}^m$$
            Therefore, we can define the vector $\vec{x}$ as;
            $$x_i = \begin{cases}
                \vec{x_B}_i & i \in I \\
                0 & i \notin I
            \end{cases}$$
            This $\vec{x}$ is the \textbf{unique basic solution} to $\mat{A}\vec{x} = \vec{b}$ with respect to $I$.
            However - this doesn't mean it's feasible, as we could end up with negative values.
            The geometric intuition that the corners of the feasible set correspond are LP come back into play, when we consider that the corners of the feasible set correspond to \textbf{basic feasible solutions}.
            \medskip

            Consider the example from the first lecture (note that each line in the previously drawn graph denotes when a variable in the standard form is zero);
            \begin{align*}
                y & = x_1 + x_2 & \text{objective function} \\
                2x_1 + x_2 & \leq 11 & \text{constraint on availability of $X$} \\
                x_1 + 3x_2 & \leq 18 & \text{constraint on availability of $Y$} \\
                x_1 & \leq 4 & \text{constraint on demand of $A$} \\
                x_1, x_2 & \geq 0 & \text{non-negativity constraints}
                \intertext{In standard form:}
                n & = 5 & \text{number of variables} \\
                m & = 3 & \text{number of constraints} \\
                z & = -x_1 - x_2 & \text{objective function} \\
                2x_1 + x_2 + x_3 & = 11 \\
                x_1 + 3x_2 + x_4 & = 18 \\
                x_1 + x_5 & = 4 \\
                x_1, x_2, x_3, x_4, x_5 & \geq 0
            \end{align*}
            \begin{center}
                \begin{tikzpicture}[x=0.5cm, y=0.5cm]
                    \draw
                    (0, -1) edge[->, left] node{$x_2$} (0, 15)
                    (-1, 0) edge[->, below] node{$x_1$} (20, 0);

                    \draw[fill=black!20] (0, 0) -- (0, 6) -- (3, 5) -- (4, 3) -- (4, 0) -- cycle;

                    \draw
                    (0, 0) edge[line width=1.5pt] (0, 14)
                    (0, 0) edge[line width=1.5pt] (19, 0)
                    (0, 11) edge[line width=1.5pt] (5.5, 0)
                    (0, 6) edge[line width=1.5pt] (18, 0)
                    (4, 0) edge[line width=1.5pt] (4, 14);

                    \node[rbtb] (a) at (0, 0) {};
                    \node[rbtb] (b) at (0, 6) {};
                    \node[rbtb] (c) at (3, 5) {};
                    \node[rbtb] (d) at (4, 3) {};
                    \node[rbtb] (e) at (4, 0) {};

                    \node[rbtr] (f) at (0, 11) {};
                    \node[rbtr] (g) at (4, 14/3) {};
                    \node[rbtr] (h) at (5.5, 0) {};
                    \node[rbtr] (i) at (18, 0) {};

                    \node (bfs) at (-3, 2) {\shortstack{basic\\feasible\\solutions}};
                    \node (bis) at (10, 7) {\shortstack{basic\\infeasible\\solutions}};

                    \draw
                    (bfs) edge[->, dashed] (a)
                    (bfs) edge[->, dashed] (b)
                    (bfs) edge[->, dashed] (c)
                    (bfs) edge[->, dashed] (d)
                    (bfs) edge[->, dashed] (e)
                    (bis) edge[->, dashed] (f)
                    (bis) edge[->, dashed] (g)
                    (bis) edge[->, dashed] (h)
                    (bis) edge[->, dashed] (i);
                \end{tikzpicture}
            \end{center}
            The intuition is that the vertices of the feasible set are the basic feasible solutions.
            Therefore, an optimum is always at a vertex in geometry, hence an optimum is always achieved at a \textbf{BFS} in algebra.
            \medskip

            For an LP in standard form with $\text{rk}(\mat{A}) = m \leq n$;
            \begin{enumerate}[1.]
                \itemsep0em
                \item if there exists a feasible solution, there exists a BFS
                \item if there exists an optimal solution, there exists an optimal BFS
            \end{enumerate}
            However, there may be feasible / optimal solutions that are not BFS.
            \medskip

            The first theorem reduces solving a LP to searching over BFS's, there are a finite number of ways to select $m$ columns for $I$ for an LP in standard form with $n$ variables and $m$ constraints;
            $$\binom{n}{m} = \frac{n!}{m! (n - m)!}$$
            This gives an obvious, but very inefficient, method through a finite search.
            The number of distinct BFS is usually less than that upper bound however, as $B(I)$ may be singular (non-invertible), or the corresponding BS may not be feasible.
            \subsubsection*{Example}
                Consider the following optimisation problem;
                $$\begin{matrix}
                    \text{maximise} & y = 3x_1 + 4x_2 \\
                    \text{subject to} & x_1 + x_2 \leq 4 \\
                    & 2x_1 + x_2 \leq 5 \\
                    & x_1, x_2 \geq 0
                \end{matrix}$$
                This has the following graphical representation;
                \begin{center}
                    \begin{tikzpicture}[x=1.5cm, y=0.75cm]
                        \draw[fill=black!20] (0, 0) -- (0, 4) -- (1, 3) -- (2.5, 0) -- cycle;
                        \draw
                        (0, 4) edge[dashed] (16/3, 0)
                        (0, -1) edge[->, left] node{$x_2$} (0, 6)
                        (-1, 0) edge[->, below] node{$x_1$} (6, 0)
                        (0, 4) edge[line width=1.5pt] (4, 0)
                        (0, 5) edge[line width=1.5pt] (2.5, 0);
                    \end{tikzpicture}
                \end{center}
                We then want to convert this into standard form as follows;
                $$\begin{matrix}
                    \text{minimise} & z = -3x_1 - 4x_2 \\
                    \text{subject to} & x_1 + x_2 + x_3 = 4 \\
                    & 2x_1 + x_2 + x_4 = 5 \\
                    & x_1, x_2, x_3, x_4 \geq 0
                \end{matrix}$$
                From this, we have 4 columns, and let us choose our index set $I = \{1, 2\}$.
                Therefore;
                \begin{align*}
                    \mat{B} & = \begin{bmatrix}
                        1 & 1 \\
                        2 & 1
                    \end{bmatrix} \\
                    \mat{B}^{-1} & = \begin{bmatrix}
                        -1 & 1 \\
                        2 & -1
                    \end{bmatrix} \\
                    \vec{x_B} & = \mat{B}^{-1}\vec{b} \\
                    & = \begin{bmatrix}
                        -1 & 1 \\
                        2 & -1
                    \end{bmatrix} \begin{bmatrix}
                        4 \\ 5
                    \end{bmatrix} \\
                    & = \begin{bmatrix}
                        1 \\ 3
                    \end{bmatrix} \\
                    \vec{x} & = \begin{bmatrix}
                        1 \\ 3 \\ 0 \\ 0
                    \end{bmatrix} & \text{this is a basic feasible solution}
                \end{align*}
                With a fixed index set $I$ where $|I| = m$ and $B(I)$ invertible.
                The variables $\{x_i\}_{i \in I}$ are referred to as basic variables, while the other variables $\{x_i\}_{i \notin I}$ are referred to as the nonbasic variables corresponding to $I$.
                Nonbasic variables are \textbf{always} zero, but the basic variables can be anything (including zero).
                \medskip

                The \textbf{basic representation} corresponding to $I$ is the unique reformulation of the system $z = \vec{c}^\top\vec{x}$ and $\mat{A}\vec{x} = \vec{b}$, which expresses the objective function value $z$ and each basic variable as a linear function of the nonbasic variables;
                $$\begin{bmatrix}
                    z \\ \vec{x_B}
                \end{bmatrix} = f(\vec{x_N})$$
                \begin{itemize}
                    \itemsep0em
                    \item $\vec{x_B} = [x_i\ \vline\ i \in I]$ (basic variable)
                    \item $\vec{x_N} = [x_i\ \vline\ i \notin I]$ (nonbasic variable)
                    \item $f : \mathbb{R}^{n - m} \to \mathbb{R}^{m + 1}$ is \textbf{linear}
                \end{itemize}
                Once again, let $\mat{A} = [\vec{a_1}, \dots, \vec{a_n}]$. where $\vec{a_i} \in \mathbb{R}^m$ is the $i^\text{th}$ column of $\mat{A}$.
                Take any index set $I \subseteq \{1, \dots, m\}$ with $|I| = m$, we can define the following;
                \begin{align*}
                    \mat{B} & = [\vec{a_i}\ \vline\ i \in I] \\
                    \mat{N} & = [\vec{a_i}\ \vline\ i \notin I] \\
                    \vec{c_B} & = [c_i\ \vline\ i \in I] \\
                    \vec{c_N} & = [c_i\ \vline\ i \notin I] \\
                    \vec{x_B} & = [x_i\ \vline\ i \in I] \\
                    \vec{x_N} & = [x_i\ \vline\ i \notin I] \\
                    \intertext{This implies that;}
                    \mat{A}\vec{x} & = \mat{B}\vec{x_B} + \mat{N}\vec{x_N} \\
                    \vec{c}^\top\vec{x} & = \vec{c_B}^\top\vec{x_B} + \vec{c_N}^\top\vec{x_N}
                \end{align*}
                Given this partition, we have the following;
                $$\left.\begin{aligned}
                    z & = \vec{c}^\top\vec{x} \\
                    \mat{A}\vec{x} & = \vec{b}
                \end{aligned}\right\rbrace \Leftrightarrow \left\lbrace\begin{aligned}
                    z & = \vec{c_B}^\top\vec{x_B} + \vec{c_N}^\top\vec{x_N} \\
                    \mat{B}\vec{x_B} & = \vec{b} - \mat{N}\vec{x_N}
                \end{aligned}\right.$$
                Since $\mat{B}$ is invertible, we can get the following;
                \begin{align*}
                    \vec{x_B} & = \mat{B}^{-1}(\vec{b} - \mat{N}\vec{x_N}) \\
                    & = \mat{B}^{-1}\vec{b} - \mat{B}^{-1}\mat{N}\vec{x_N} \\
                    z & = \vec{c_B}^\top\vec{x_B} + \vec{c_N}^\top\vec{x_N} \\
                    & = \vec{c_B}^\top(\mat{B}^{-1}\vec{b} - \mat{B}^{-1}\mat{N}\vec{x_N}) + \vec{c_N}^\top\vec{x_N} \\
                    & = \vec{c_B}^\top\mat{B}^{-1}\vec{b} + (\vec{c_N}^\top - \vec{c_B}^\top\mat{B}^{-1}\mat{N})\vec{x_N} \\
                    & = \vec{c_B}^\top\mat{B}^{-1}\vec{b} + (\vec{c_N} - \mat{N}^\top\mat{B}^{-\top}\vec{c_B})^\top\vec{x_N} & \text{where } \mat{B}^{-\top} = (\mat{B}^{-1})^\top
                \end{align*}
                Therefore, the basic representation is as follows;
                \begin{align*}
                    z & = \vec{c_B}^\top\mat{B}^{-1}\vec{b} + (\vec{c_N} - \mat{N}^\top\mat{B}^{-\top}\vec{c_B})^\top\vec{x_N} \\
                    \vec{x_B} & = \mat{B}^{-1}\vec{b} - \mat{B}^{-1}\mat{N}\vec{x_N}
                \end{align*}
                This expresses $z$ and $\vec{x_B}$ as linear functions of $\vec{x_N}$.
                However by setting $\vec{x_N} = \vec{0}$, we obtain the basic solution $\vec{x} = (\vec{x_B}, \vec{x_N}) = (\mat{B}^{-1}\vec{b}, \vec{0})$, with the objective value $z = \vec{c_B}^\top\mat{B}^{-1}\vec{b}$.
                The \textbf{reduced cost vector} is $\vec{r} = \vec{c_N} - \mat{N}^\top\mat{B}^{-\top}\vec{c_B}$, which characterises the sensitivity of the objective function value $z$ with respect to the nonbasic variables.
                \medskip

                Referring back to the previous example;
                $$\begin{matrix}
                    \text{minimise} & z = -3x_1 - 4x_2 \\
                    \text{subject to} & x_1 + x_2 + x_3 = 4 \\
                    & 2x_1 + x_2 + x_4 = 5 \\
                    & x_1, x_2, x_3, x_4 \geq 0
                \end{matrix}$$
                Consider the solution we get when;
                \begin{align*}
                    I & = \{3, 4\} \\
                    O & = (0, 0, 4, 5) \\
                    z & = -3x_1 - 4x_2 \\
                    x_3 & = 4 - x_1 - x_2 \\
                    x_4 & = 5 - 2x_1 - x_2
                \end{align*}
                However, by looking at the objective function, we can see that it is more desirable to increase the value of $x_2$, so we fix $x_1 = 0$.
                This then gives us the following;
                \begin{align*}
                    z & = -4x_2 \\
                    x_3 & = 4 - x_2 \\
                    & \geq 0 \\
                    x_4 & = 5 - x_2 \\
                    & \geq 0 & \Rightarrow \\
                    x_2 & \leq 4 & \Rightarrow \\
                    x_2 & = 4 & \Rightarrow \\
                    x_3 & = 0
                \end{align*}
                This \textbf{pivoting} changes the index set to be $I = \{2, 4\}$.
                Looking at the nonbasic variables $\{x_1, x_3\}$;
                \begin{align*}
                    z & = -3x_1 - 4x_2 \\
                    & = -3x_1 - 4(\red{4 - x_1 - x_3}) \\
                    & = -3x_1 - 16 + 4x_1 + 4x_3 \\
                    & = -16 + x_1 + 4x_3 \\
                    \red{x_2} & = \red{4 - x_1 - x_3}
                \end{align*}
                We can see, by looking at the coefficients, that $x_1$ and $x_3$ will cause the minimal solution to increase if they weren't zero.
\end{document}