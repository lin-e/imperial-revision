\documentclass[a4paper, 12pt]{article}

% packages
\usepackage{amssymb}
\usepackage[fleqn]{mathtools}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{bussproofs}
\usepackage{xcolor}
\usepackage[margin=1.3cm]{geometry}
\usepackage{logicproof}
\usepackage{diagbox}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{lstautogobble}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{tipa}
\usepackage{pgfplots}
\usepackage{adjustbox}

% tikz libraries
\usetikzlibrary{
    decorations.pathreplacing,
    arrows,
    shapes,
    shapes.gates.logic.US,
    circuits.logic.US,
    calc,
    automata,
    positioning,
    intersections
}

\pgfplotsset{compat=1.16}

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\allowdisplaybreaks % allow environments to break
\setlength\parindent{0pt} % no indent

% shorthand for verbatim
% this clashes with logicproof, so maybe fix this at some point?
\catcode`~=\active
\def~#1~{\texttt{#1}}

% code listing
\lstdefinestyle{main}{
    numberstyle=\tiny,
    breaklines=true,
    showspaces=false,
    showstringspaces=false,
    tabsize=2,
    numbers=left,
    basicstyle=\ttfamily,
    columns=fixed,
    fontadjust=true,
    basewidth=0.5em,
    autogobble,
    xleftmargin=3.0ex,
    mathescape=true
}
\newcommand{\dollar}{\mbox{\textdollar}} %
\lstset{style=main}

% augmented matrix
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
\hskip -\arraycolsep
\let\@ifnextchar\new@ifnextchar
\array{#1}}
\makeatother

% ceiling / floor
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% custom commands
\newcommand{\indefint}[2]{\int #1 \, \mathrm{d}#2}
\newcommand{\defint}[4]{\int_{#1}^{#2} #3 \, \mathrm{d}#4}
\newcommand{\pdif}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dif}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\newcommand{\limit}[2]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle{\lim_{#1 \to #2}}$}}}
\newcommand{\limitsup}[2]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle{\limsup_{#1 \to #2}}$}}}
\newcommand{\summation}[2]{\sum\limits_{#1}^{#2}}
\newcommand{\product}[2]{\prod\limits_{#1}^{#2}}
\newcommand{\intbracket}[3]{\left[#3\right]_{#1}^{#2}}
\newcommand{\laplace}{\mathcal{L}}
\newcommand{\fourier}{\mathcal{F}}
\newcommand{\mat}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\rowt}[1]{\begin{bmatrix}
    #1
\end{bmatrix}^\top}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\lto}[0]{\leadsto\ }

\newcommand{\ulsmash}[1]{\underline{\smash{#1}}}

\newcommand{\powerset}[0]{\wp}
\renewcommand{\emptyset}[0]{\varnothing}

\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{\(\m@th{#1\langle}\)}%
  \mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{\(\m@th{#1\rangle}\)}%
  \mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother
\newcommand{\lla}{\llangle}
\newcommand{\rra}{\rrangle}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\crnr}[1]{\text{\textopencorner} #1 \text{\textcorner}}
\newcommand{\bnfsep}[0]{\ |\ }
\newcommand{\concsep}[0]{\ ||\ }

\newcommand{\axiom}[1]{\AxiomC{#1}}
\newcommand{\unary}[1]{\UnaryInfC{#1}}
\newcommand{\binary}[1]{\BinaryInfC{#1}}
\newcommand{\trinary}[1]{\TrinaryInfC{#1}}
\newcommand{\quaternary}[1]{\QuaternaryInfC{#1}}
\newcommand{\quinary}[1]{\QuinaryInfC{#1}}
\newcommand{\dproof}[0]{\DisplayProof}
\newcommand{\llabel}[1]{\LeftLabel{\scriptsize #1}}
\newcommand{\rlabel}[1]{\RightLabel{\scriptsize #1}}

\newcommand{\ttbs}{\char`\\}
\newcommand{\lrbt}[0]{\ \bullet\ }

% colours
\newcommand{\violet}[1]{\textcolor{violet}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\teal}[1]{\textcolor{teal}{#1}}

% reasoning proofs
\usepackage{ltablex}
\usepackage{environ}
\keepXColumns
\NewEnviron{reasoning}{
    \begin{tabularx}{\textwidth}{rlX}
        \BODY
    \end{tabularx}
}
\newcommand{\proofline}[3]{$(#1)$ & $#2$ & \hfill #3 \smallskip \\}
\newcommand{\proofarbitrary}[1]{& take arbitrary $#1$ \smallskip \\}
\newcommand{\prooftext}[1]{\multicolumn{3}{l}{#1} \smallskip \\}
\newcommand{\proofmath}[3]{$#1$ & = $#2$ & \hfill #3 \smallskip \\}
\newcommand{\prooftherefore}[1]{& $\therefore #1$ \smallskip \\}
\newcommand{\proofbc}[0]{\prooftext{\textbf{Base Case}}}
\newcommand{\proofis}[0]{\prooftext{\textbf{Inductive Step}}}

% ER diagrams
\newcommand{\nattribute}[4]{
    \node[draw, state, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\mattribute}[4]{
    \node[draw, state, accepting, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\dattribute}[4]{
    \node[draw, state, dashed, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\entity}[3]{
    \node[] (#1-c) at (#2) {#3};
    \node[inner sep=0cm] (#1-l) at ($(#1-c) + (-1, 0)$) {};
    \node[inner sep=0cm] (#1-r) at ($(#1-c) + (1, 0)$) {};
    \node[inner sep=0cm] (#1-u) at ($(#1-c) + (0, 0.5)$) {};
    \node[inner sep=0cm] (#1-d) at ($(#1-c) + (0, -0.5)$) {};
    \draw
    ($(#1-c) + (-1, 0.5)$) -- ($(#1-c) + (1, 0.5)$) -- ($(#1-c) + (1, -0.5)$) -- ($(#1-c) + (-1, -0.5)$) -- cycle;
}
\newcommand{\relationship}[3]{
    \node[] (#1-c) at (#2) {#3};
    \node[inner sep=0cm] (#1-l) at ($(#1-c) + (-1, 0)$) {};
    \node[inner sep=0cm] (#1-r) at ($(#1-c) + (1, 0)$) {};
    \node[inner sep=0cm] (#1-u) at ($(#1-c) + (0, 1)$) {};
    \node[inner sep=0cm] (#1-d) at ($(#1-c) + (0, -1)$) {};
    \draw
    ($(#1-c) + (-1, 0)$) -- ($(#1-c) + (0, 1)$) -- ($(#1-c) + (1, 0)$) -- ($(#1-c) + (0, -1)$) -- cycle;
}

% AVL Trees
\newcommand{\avltri}[4]{
    \draw ($(#1)$) -- ($(#1) + #4*(0.5, -1)$) -- ($(#1) + #4*(-0.5, -1)$) -- cycle;
    \node at ($(#1) + #4*(0, -1) + (0, 0.5)$) {#3};
    \node at ($(#1) + #4*(0, -1) + (0, -0.5)$) {#2};
}

% RB Trees
\tikzset{rbtr/.style={inner sep=2pt, circle, draw=black, fill=red}}
\tikzset{rbtb/.style={inner sep=2pt, circle, draw=black, fill=black}}

% Samples
\tikzset{spos/.style={inner sep=2pt, circle, draw=black, fill=blue!20}}
\tikzset{sneg/.style={inner sep=2pt, circle, draw=black, fill=red!20}}

% Joins
\newcommand\ljoin{\stackrel{\mathclap{\normalfont\mbox{\tiny L}}}{\bowtie}}
\newcommand\rjoin{\stackrel{\mathclap{\normalfont\mbox{\tiny R}}}{\bowtie}}
\newcommand\ojoin{\stackrel{\mathclap{\normalfont\mbox{\tiny O}}}{\bowtie}}

% actual document
\begin{document}
    {\sc Computing $3^\text{rd}$ Year Notes} \hfill ~https://github.com/lin-e/imperial-revision~
    \rule{\textwidth}{0.1pt}
    \section*{CO572 - Advanced Databases \hfill (60002)}
        \subsection*{Lecture 1}
            What is a \textbf{database management system}?
            A \textbf{database} is any structured collection of data points, which can be a relational table, a set, a vector, a graph, or anything along those lines.
            \textbf{Data management} is needed for \textbf{data-intensive applications}, we say something processes a significant amount of data if the amount of data is larger than what fits in the CPU's cache, something in the order of a few MB.
            We can say that below this threshold (around 5MB - 50MB), there are other factors that likely dominate performance.
            A \textbf{system} is made up from components that interact together to achieve a greater goal and is usually applicable to many situations.
            \subsubsection*{Applications}
                \begin{itemize}
                    \itemsep0em
                    \item
                        The scenario is at a hospital.
                        At any given time, there are 800 patients, producing a sample per second of 5 metrics.
                        There are also 200 doctors and nurses, who each produce a textual report every 10 minutes, and 80 lab technicians producing a structured dataset of 10 metrics every 5 minutes.
                        Everything must be stored \textbf{reliably}, it cannot be lost after it is stored (or with a probability $p < 0.001$).
                    \item
                        You are developing a interactive dashboard for a global retail company.
                        This company has stored 500GBs of sales, inventory, and customer records, and shall provide interactive access to calculated statistics.
                        This should allow filtering of the dataset with predicates, and support the calculation of the sums of records, all with response times below a second.
                \end{itemize}
                Below are some examples of typical data-intensive application patterns;
                \begin{itemize}
                    \itemsep0em
                    \item Online Transaction Processing \hfill \textbf{OLTP}
                        \begin{itemize}
                            \itemsep0em
                            \item lots of small updates to a persistent database
                            \item focused on throughput (do as many updates as possible in a time-frame)
                            \item ACID is key (reliable)
                        \end{itemize}
                    \item Online Analytical Processing \hfill \textbf{OLAP}
                        \begin{itemize}
                            \itemsep0em
                            \item running a single data analysis task
                            \item focus on latency (do queries as quickly as possible)
                            \item \textbf{ad-hoc} queries - we don't know what they'll be
                        \end{itemize}
                    \item Reporting
                        \begin{itemize}
                            \itemsep0em
                            \item running many analysis tasks in a fixed time budget
                            \item focused on resource efficiency - if we can do the same task by the same time it's due with fewer resources, then it would be cheaper
                            \item queries are known in advance (and can be compiled into the system)
                        \end{itemize}
                    \item Hybrid Transactional / Analytical Processing \hfill \textbf{HTAP}
                        \begin{itemize}
                            \itemsep0em
                            \item a mix of \textbf{OLTP} and \textbf{OLAP}
                            \item small updates woven with larger analytics
                            \item common application is fraud detection
                        \end{itemize}
                \end{itemize}
            \subsubsection*{Data-intensive Application vs Management System}
                A \textbf{application} is not generic - it's often domain-specific with the logic baked in, and is therefore hard to generalise to other applications.
                Generally, the cost (such as adapting for other applications) of application-specific data management outweighs the benefits.
                We can generalise the following (from an application to a management system);
                \begin{itemize}
                    \itemsep0em
                    \item \textit{Yelp}
                    \item mobile app for geo-services
                    \item library to manage unordered collections of tagged coordinates
                    \item spatial data management library
                    \item relational database
                    \item block storage system
                \end{itemize}
            \subsubsection*{Requirements of a Data Management System}
                A data management system should fulfil the following requirements;
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{efficiency} \hfill should not be significantly slower than hand written applications
                    \item \textbf{resilience} \hfill should recover from problems, such as power outage, hardware or software failures
                    \item \textbf{robustness}
                        \subitem should have predictable performance; a small change in the query should not lead to major changes in performance
                    \item \textbf{scalability}
                        \subitem should make efficient use of available resources - increase in resources should lead to an improvement of performance
                    \item \textbf{concurrency}
                        \subitem should transparently serve multiple clients transparently (without impacting results)
                \end{itemize}
            \subsubsection*{Solutions}
                DBMSs often provide some ingenious solutions;
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{Physical and Logical Data Model Separation}
                        \subitem
                            We typical provide a \textbf{logical data model} to the user, as they often send data in a \textbf{fire and forget} manner.
                            The user doesn't typically care about file format, storage devices, nor portability.
                            The DMBSs can then separate external from the internal model and therefore exploit these degrees of freedom for performance.
                            For example, if the user doesn't care where the data is stored, we can keep the hot data on an SSD and the cold data on disk or even tape.
                    \item \textbf{Transparent Concurrency: Transactional Semantics}
                        \begin{itemize}
                            \itemsep0em
                            \item \textbf{Atomic} \hfill run completely or not at all (if aborted, everything is reverted)
                            \item \textbf{Consistent} \hfill constraints must hold before and after (can be inconsistent between)
                            \item \textbf{Isolated} \hfill run like you were alone on the system
                            \item \textbf{Durable} \hfill after a transaction is committed \textbf{nothing} can undo it
                        \end{itemize}
                    \item \textbf{Ease of Use: Declarative Data Analysis}
                        \subitem
                            Retrieving information about data should be done by describing the result and the system will generate it.
                            This can be a single tuple, some statistics, a detailed generated report, or even training a model to predict data.
                \end{itemize}
                However, they are not to be used as a filesystem.
                Similarly, it cannot be used as runtime for applications - there are support for user-defined functions, but should not be used a such.
                They also should not be used to store intermediate data (such as whether a user is logged in).
            \subsubsection*{Relational Algebra}
                A schema is the definition of the attributes of the tuples in the relations, but also can contain integrity constraints.
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{vector} \hfill ordered collection of objects of the same type
                    \item \textbf{tuple} \hfill ordered collection of objects of different type
                    \item \textbf{bag} \hfill unordered collection of objects of the same type
                    \item \textbf{set} \hfill unordered collection of unique objects of the same type
                \end{itemize}
                Relational algebra is used to define the semantics of operations and is used for logical optimisation.
                However, it's not actually that useful for end-users.
                A relation is an array which represents an $n$-ary relation $R$, with the following properties;
                \begin{itemize}
                    \itemsep0em
                    \item each row represents an $n$-tuple of $R$
                    \item the order of rows doesn't matter
                    \item all rows are distinct (combined with above is a set)
                    \item the order of columns matters - all rows have the same schema
                    \item each column has a label (defines the schema of our relation)
                \end{itemize}
                Relations are \textbf{almost} sets of tuples.
                A rough implementation in C++ is as follows;
                \begin{lstlisting}
                    template <typename... types>
                    struct Relation {
                        using OutputType = tuple<types...>;
                        set<tuple<types...>> data;
                        array<string, sizeof...(types)> schema;
                        Relation(){};
                        Relation(array<string, sizeof...(types)> schema, set<tuple<types...>> data): schema(schema), data(data) {}
                    };
                \end{lstlisting}
                A relation can then be written as follows;
                \begin{lstlisting}
                    auto createCustomerTable() {
                        Relation<int, string, string> customer(
                            {"ID", "Name", "ShippingAddress"}, // labels of the attribute
                            {{1, "james", "address 1"},
                             {2, "steve", "another address}});
                        return customer;
                    }
                \end{lstlisting}
                A \textbf{relational expression} is composed from \textbf{relational operators}, and will often be referred to as a \textbf{(logical) plan}.
                \textbf{Cardinality} is the number of tuples in a set.
                Relational operations are set-based, and therefore order-invariant and duplicates are eliminated.
                Additionally, it's \textbf{closed};
                \begin{itemize}
                    \itemsep0em
                    \item every operator produces a relation as an output
                    \item every operator accepts one or two relations as input
                    \item simplifies the composition of operators into expressions (however expressions can be invalid)
                \end{itemize}
                Relational operators can be implemented as follows;
                \begin{lstlisting}
                    template <typename... types> struct Operator : public Relation<types...> {}; // therefore an operator is a relation
                \end{lstlisting}
                A minimal set of relational operators is as follows;
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{project} ($\pi$)
                        \begin{itemize}
                            \itemsep0em
                            \item extract one or more attributes from a relation
                            \item preserves relational semantics
                            \item changes schema
                        \end{itemize}
                        For example, given a table;
                        \begin{center}
                            \hfill
                            \begin{tabular}{ccc}
                                \multicolumn{3}{c}{~table1~} \\
                                ~field1~ & ~field2~ & ~field3~ \\
                                \hline
                                ~A~ & ~B~ & ~C~ \\
                                ~D~ & ~E~ & ~F~ \\
                                ~G~ & ~E~ & ~I~
                            \end{tabular}
                            \hfill
                            \begin{tabular}{c}
                                \multicolumn{1}{c}{$\pi_~field2~~table1~$} \\
                                ~field2~ \\
                                \hline
                                ~B~ \\
                                ~E~ \\
                                \phantom{}
                            \end{tabular}
                            \hfill \phantom{}
                        \end{center}
                        The cardinality of the output of a projection can only be determined by evaluating it, as duplicates can be eliminated.
                        On the other hand, the upper bound of the cardinality of the output is the cardinality of the input.
                        \medskip

                        It can also extract one or more attributes from a relation and perform a scalar operation on them.
                        \begin{lstlisting}
                            template <typename InputOperator, typename... outputTypes>
                            struct Project : public Operator<outputTypes...> {
                                InputOperator input;

                                variant<function<tuple<outputTypes...>(typename InputOperator::OutputType)>,
                                        set<pair<string, string>>>
                                    projections;

                                Project(InputOperator input, function<tuple<outputTypes...>(typename InputOperator::OutputType)> projections : input(input), projections(projections) {};

                                Project(InputOperator input, set<pair<string, string>> projections : input(input), projections(projections) {};
                            };

                            void projectionExample {
                                auto customer = createCustomerTable();

                                auto p1 = Project<decltype(customer), string>(customer, [](auto input) { return get<1>(input); });

                                auto p2 = Project<decltype(customer), string>(customer, {{"Name", "customerName"}});
                            }
                        \end{lstlisting}
                    \item \textbf{select} ($\sigma$)
                        \begin{itemize}
                            \itemsep0em
                            \item produces a new relation containing tuples which satisfy a condition
                            \item does not change schema
                            \item changes cardinality (number of tuples in a relation)
                        \end{itemize}
                        For example, given a table;
                        \begin{center}
                            \hfill
                            \begin{tabular}{ccc}
                                \multicolumn{3}{c}{~table1~} \\
                                ~field1~ & ~field2~ & ~field3~ \\
                                \hline
                                ~A~ & ~B~ & ~C~ \\
                                ~D~ & ~E~ & ~F~ \\
                                ~G~ & ~E~ & ~I~
                            \end{tabular}
                            \hfill
                            \begin{tabular}{ccc}
                                \multicolumn{3}{c}{$\sigma_~field2=E~~table1~$} \\
                                ~field1~ & ~field2~ & ~field3~ \\
                                \hline
                                ~D~ & ~E~ & ~F~ \\
                                ~G~ & ~E~ & ~I~ \\
                                \phantom{}
                            \end{tabular}
                            \hfill \phantom{}
                        \end{center}
                        The cardinality can only be determined by evaluating it, and the upper bound of the cardinality is also the cardinality of the input.
                        \begin{lstlisting}
                            enum class Comparator { less, lessEqual, equal, greaterEqual, greater };

                            struct Column {
                                string name;
                                Column(string name) : name(name) {};
                            };
                            using Value = variant<string, int float>;

                            struct Condition {
                                Column leftHandSide;
                                Comparator compare;
                                variant<Column, Value> rightHandSide;

                                Condition(Column leftHandSide, Comparator compare, variant<Column, Value> rightHandSide): leftHandSide(leftHandSide), compare(compare), rightHandSide(rightHandSide) {};
                            };
                        \end{lstlisting}
                    \item \textbf{cross product} ($\times$)
                        \begin{itemize}
                            \itemsep0em
                            \item takes two inputs
                            \item produces a new relation by combining every tuple from the left with every tuple from the right
                            \item changes the schema
                        \end{itemize}
                        For example, given tables;
                        \begin{center}
                            \hfill
                            \begin{tabular}{ccc}
                                \multicolumn{2}{c}{~table1~} \\
                                ~field1~ & ~field2~ \\
                                \hline
                                ~A~ & ~B~ \\
                                ~G~ & ~E~ \\
                                \phantom{} \\
                                \phantom{}
                            \end{tabular}
                            \hfill
                            \begin{tabular}{ccc}
                                \multicolumn{2}{c}{~table2~} \\
                                ~fieldA~ & ~fieldB~ \\
                                \hline
                                ~2~ & ~6~ \\
                                ~5~ & ~1~ \\
                                \phantom{} \\
                                \phantom{}
                            \end{tabular}
                            \hfill
                            \begin{tabular}{cccc}
                                \multicolumn{4}{c}{$~table1~ \times ~table2~$} \\
                                ~field1~ & ~field2~ & ~fieldA~ & ~fieldB~ \\
                                \hline
                                ~A~ & ~B~ & ~2~ & ~6~ \\
                                ~A~ & ~B~ & ~5~ & ~1~ \\
                                ~G~ & ~E~ & ~2~ & ~6~ \\
                                ~G~ & ~E~ & ~5~ & ~1~
                            \end{tabular}
                            \hfill \phantom{}
                        \end{center}
                        The cardinality of the output is the product of the two input cardinalities.
                        \begin{lstlisting}
                            template <typename LeftInputOperator, typename RightInputOperator>
                            struct CrossProduct : public Operator<Concat<typename LeftInputOperator::OutputType, typename RightOutputOperator::OutputType>> {
                                LeftInputOperator leftInput;
                                RightInputOperator rightInput;
                                CrossProduct(LeftInputOperator leftInput, RightInputOperator  rightInput): leftInput(leftInput), rightInput(rightInput) {};
                            };
                        \end{lstlisting}
                    \item \textbf{union} ($\cup$)
                        \begin{itemize}
                            \itemsep0em
                            \item produces a new relation from two relations containing any tuple that is present in either
                            \item does not change the schema (but does require schema compatibility)
                            \item changes cardinality
                        \end{itemize}
                        No example provided, because it's quite simple.
                        \begin{lstlisting}
                            template <typename LeftInputOperator, typename RightInputOperator>
                            struct Union : public Operator<typename LeftInputOperator:::OutputType> {
                                LeftInputOperator leftInput;
                                RightInputOperator rightInput;

                                Union(LeftInputOperator leftInput, RightInputOperator rightInput): leftInput(leftInput), rightInput(rightInput){};
                            };
                        \end{lstlisting}
                        The cardinality can only be known by evaluating (due to duplicates), and the upper bound is the sum of the cardinalities of the input.
                    \item \textbf{difference} ($-$)
                        \begin{itemize}
                            \itemsep0em
                            \item produces new relation from two relations containing tuples present in the first but not the second
                            \item doesn't change schema (requires compatibility)
                            \item changes cardinality
                        \end{itemize}
                        No example provided, because it's quite simple.
                        \begin{lstlisting}
                            template <typename LeftInputOperator, typename RightInputOperator>
                            struct Difference : public Operator<typename LeftInputOperator:::OutputType> {
                                LeftInputOperator leftInput;
                                RightInputOperator rightInput;

                                Difference(LeftInputOperator leftInput, RightInputOperator rightInput): leftInput(leftInput), rightInput(rightInput){};
                            };
                        \end{lstlisting}
                    \item \textbf{group aggregation} ($\Gamma$)
                        \begin{itemize}
                            \itemsep0em
                            \item produces new relation from one input by grouping tuples that have equal values in some attributes and aggregate others - groups are defined by the set of grouping attributes (can be empty), and the aggregates are defined by the set of \textbf{aggregations} which are triples consisting of;
                                \begin{itemize}
                                    \itemsep0em
                                    \item \textbf{input} attribute
                                    \item \textbf{aggregation function} (min, max, avg, sum, count)
                                    \item \textbf{output} attribute
                                \end{itemize}
                            \item this changes both the schema and cardinality
                        \end{itemize}
                        For example, given the following table;
                        \begin{center}
                            \hfill
                            \begin{tabular}{ccc}
                                \multicolumn{3}{c}{~customer~} \\
                                ~ID~ & ~Name~ & ~City~ \\
                                \hline
                                ~1~ & ~james~ & ~London~ \\
                                ~2~ & ~steve~ & ~London~ \\
                                ~3~ & ~kate~ & ~Manchester~
                            \end{tabular}
                            \hfill
                            \begin{tabular}{cc}
                                \multicolumn{2}{c}{$\Gamma((~City~),((~ID~,~count~,~c~)))~Customer~$} \\
                                ~City~ & ~c~ \\
                                \hline
                                ~London~ & ~2~ \\
                                ~Manchester~ & ~1~ \\
                                \phantom{}
                            \end{tabular}
                            \hfill
                            \phantom{}
                        \end{center}
                        \begin{lstlisting}
                            enum class AggregationFunction { min, max, sum, avg, count };

                            template <typename InputOperator, typename... Output>
                            struct GroupedAggregation : public Operator<Output...> {
                                InputOperator input;
                                set<string> groupAttributes;
                                set<tuple<string, AggregationFunction, string>> aggregations;
                                GroupedAggregation(InputOperator input, set<string> groupAttributes, set<tuple<string, AggregationFunction, string>> aggregations): input(input), groupAttributes(groupAttributes), aggregations(aggregations){};
                            };
                        \end{lstlisting}
                    \item \textbf{Top-N} ($T$)
                        \begin{itemize}
                            \itemsep0em
                            \item produce new relation from one input selecting the tuples with the $N$ greatest values with respect to an attribute
                            \item changes cardinality, but maintains schema
                        \end{itemize}
                        For example, given the following table;
                        \begin{center}
                            \hfill
                            \begin{tabular}{ccc}
                                \multicolumn{3}{c}{~customer~} \\
                                ~ID~ & ~Name~ & ~City~ \\
                                \hline
                                ~1~ & ~james~ & ~London~ \\
                                ~2~ & ~steve~ & ~London~ \\
                                ~3~ & ~kate~ & ~Manchester~
                            \end{tabular}
                            \hfill
                            \begin{tabular}{ccc}
                                \multicolumn{3}{c}{$T_{(2, ~ID~)}~Customer~$} \\
                                ~ID~ & ~Name~ & ~City~ \\
                                \hline
                                ~2~ & ~steve~ & ~London~ \\
                                ~3~ & ~kate~ & ~Manchester~ \\
                                \phantom{}
                            \end{tabular}
                            \hfill
                            \phantom{}
                        \end{center}
                        \begin{lstlisting}
                            template <typename InputOperator>
                            struct TopN : public Operator<typename InputOperator::OutputType> {
                                InputOperator input;
                                size_t N;
                                string predicate;
                                TopN(InputOperator input, size_t N, string predicate): input(input), N(N), predicate(predicate){};
                            };
                        \end{lstlisting}
                \end{itemize}
                Since relational algebra is closed, operators can be combined as long as signatures are respected (cross product takes two inputs, whereas selections and projections take one);
                $$\pi_~BookID~(\sigma_~Order.ID == OrderedItem.OrderID~(~Order~ \times ~OrderedItem~))$$
        \subsection*{Lecture 2}
            \subsubsection*{Architecture}
                The logical plan is the relational algebra discussed last lecture and the database kernel actually interacts with the resources mentioned.
                A database kernel is a library of core functionality, including I/O, memory management, operators, etc.
                Generally, it provides an interface to subsystems (similar to an OS kernel).
                A basic database kernel architecture is as follows (assuming both catalogue and data live in memory).
                \begin{center}
                    \begin{tikzpicture}[x=1.5cm, y=0.5cm]
                        \draw
                        (0, 0) -- (9, 0) -- (9, -5) -- (0, -5) -- cycle
                        (1, -1) -- (4, -1) -- (4, -2) -- (1, -2) -- cycle
                        (5, -1) -- (8, -1) -- (8, -2) -- (5, -2) -- cycle
                        (-3, 0) -- (-1, 0) -- (-1, -2) -- (-3, -2) -- cycle
                        (-3, -3) -- (-1, -3) -- (-1, -5) -- (-3, -5) -- cycle;

                        \node at (2.5, -1.5) {storage manager};
                        \node at (6.5, -1.5) {op. implementations};
                        \node at (-2, -1) {catalogue};
                        \node at (-2, -4) {data};
                        \node at (4.5, -4) {database kernel};
                        \draw
                        (5, -1.5) edge[->] (4, -1.5)
                        (1, -1.5) edge[->] (-1, -1.5)
                        (2, -2) edge[->] (-1, -4.5);
                    \end{tikzpicture}
                \end{center}
                The DBMS architecture is as follows;
                \begin{center}
                    \begin{tikzpicture}[y=0.75cm]
                        \node (u) at (2, 1) {user};
                        \node at (2, -0.5) {external interface};
                        \node at (2, -2.5) {logical plan};
                        \node at (2, -4.5) {physical plan};
                        \node at (2, -6.5) {database kernel};
                        \draw (0, 0) -- (4, 0) -- (4, -1) -- (0, -1) -- cycle;
                        \draw (0, -2) -- (4, -2) -- (4, -3) -- (0, -3) -- cycle;
                        \draw (0, -4) -- (4, -4) -- (4, -5) -- (0, -5) -- cycle;
                        \draw (0, -6) -- (4, -6) -- (4, -7) -- (0, -7) -- cycle;
                        \node (storage) at (-1, -8) {storage};
                        \node (cpu) at (1, -8.5) {processor};
                        \node (mem) at (3, -8.5) {memory};
                        \node (cloud) at (5, -8) {cloud};
                        \draw
                        (u) edge[->] (2, 0)
                        (2, -1) edge[->] (2, -2)
                        (2, -3) edge[->] (2, -4)
                        (2, -5) edge[->] (2, -6)
                        (0.5, -7) edge[->] (storage)
                        (1.5, -7) edge[->] (cpu)
                        (2.5, -7) edge[->] (mem)
                        (3.5, -7) edge[->] (cloud);
                    \end{tikzpicture}
                \end{center}
                The kernel interface, in C++, would look like the following;
                \begin{lstlisting}
                    class StorageManager;
                    class OperatorImplementations;

                    // just provides an interface to the above
                    class DatabaseKernel {
                        StorageManager& getStorageManager();
                        OperatorImplementations& getOperatorImplementations();
                    }
                \end{lstlisting}
            \subsubsection*{Storage}
                The first operation every database needs to support inserting new tuples.
                These inserts are usually not optimised (generally) - they arrive at the storage layer as intact tuples.
                Consider the following example, in SQL;
                \begin{lstlisting}
                    CREATE TABLE Employee (
                        id int,
                        name varchar,
                        salary int,
                        joiningDate int
                    );

                    INSERT INTO Employee VALUES(1, 'james', 100000, 43429342);
                    SELECT * FROM Employee WHERE id=1;
                    DELETE FROM Employee WHERE name='james';
                \end{lstlisting}
                The equivalent in C++ would be as follows;
                \begin{lstlisting}
                    StorageManager().getTable("Employee").insert({1, "james", 100000, 43429342});
                    StorageManager().getTable("Employee").findTuplesWithAttributeValue(id, 4);
                    StorageManager().getTable("Employee").deleteTuplesWithAttributeValue(name, "james");
                \end{lstlisting}
                The storage manager interface would be as follows;
                \begin{lstlisting}
                    struct Table;
                    class StorageManager {
                        map<string, Table> catalogue;

                        public:
                            Table& getTable(string name) {return catalogue[name]; };
                    };
                \end{lstlisting}
                In order to store tuples, we need to decide where and how we store data.
                Generally, for where we store it, we mainly consider disk or memory.
                The way we store data can vary from being sorted, compressed, RLE-encoded, etc.
                A simple table interface could be as follows;
                \begin{lstlisting}
                    struct Table {
                        using AttributeValue = variant<int, float, string>;
                        using Tuple = vector<AttributeValue>;

                        void insert(Tuple) {};
                        vector<Tuple> findTuplesWithAttributeValue(int attributePosition, AttributeValue value) {};
                        void deleteTuplesWithAttributeValue(int attributePosition, AttributeValue value) {};
                    };
                \end{lstlisting}
                There is a fundamental mismatch in storing tuples; relations are two-dimensional, whereas memory is one-dimensional, therefore tuples need to be linearized in one of the two mainstream strategies;
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{The N-ary Storage Model} (NSM)
                        \subitem
                            In this model, the entries added are stored one after the other, in a one-to-one mapping onto memory.
                            This is also referred to as a \textbf{row store}.
                            An example of this is as follows;
                            \begin{lstlisting}
                                struct NSMTable : public Table {

                                    // not the tuple exposed to the outside
                                    struct InternalTuple {
                                        Tuple actualTuple;
                                        bool deleted = false;
                                        InternalTuple(Tuple t) : actualTuple(t) {};
                                    }
                                    vector<InternalTuple> data;

                                    void insert(Tuple);
                                    vector<Tuple> findTuplesWithAttributeValue(int attributePosition, AttributeValue value) {};
                                    void deleteTuplesWithAttributeValue(int attributePosition, AttributeValue value) {};
                                };

                                // declared as member of a class (just append to a vector)
                                void NSMTable::insert(Tuple t) { data.push_back(t); }

                                vector<Table::Tuple> NSMTable::findTuplesWithAttributeValue(int attributePosition, AttributeValue value) {
                                    vector<Tuple> result;
                                    for (size_t i = 0; i < data.size(); i++) {
                                        if (data[i].actualTuple[attributePosition] == value && !data[i].deleted) {
                                            result.push_back(data[i].actualTuple);
                                        }
                                    }
                                    return result;
                                }

                                // we don't want to move everything therefore we just mark them as deleted
                                void NSMTable::deleteTuplesWithAttributeValue(int attributePosition, AttributeValue value) {
                                    for (size_t i = 0; i < data.size(); i++) {
                                        if (data[i].actualTuple[attributePosition] == value) {
                                            data[i].deleted = true;
                                        }
                                    }
                                }
                            \end{lstlisting}
                            However, this may not always be a good idea, as operations will require us going over the entire vector.
                            Recall that we are assuming everything is in memory, however locality still matters (since the data may be further apart depending on the size of the tuple).
                            \medskip

                            Memory is organised in \textbf{cache lines} (usually 64 bytes in size).
                            When a core needs something, it asks L1, L2, and L3 cache first, before going to memory, and retrieves the entire cache line that the data resides on (and cache into L1 cache).
                            If we then access something on the same cache line, then the access is almost free (in terms of time).
                            More tuples will therefore fit on a single cache line if they are smaller.
                            Cache lines can also be referred to as \textbf{blocks} or \textbf{pages}.
                            \medskip

                            N-ary storage works well in inserting a new tuple, when we are inserting a tuple onto the same cache line.
                            It also works well when we want to retrieve a tuple by its index.
                            On the other hand, it's suboptimal when we want to check every row, which in the worst case will be across different cache lines.
                    \item \textbf{Decomposed Storage Model} (DSM)
                        \subitem
                            On the other hand, in this model, entries are stored with the columns one after the other.
                            This is also referred to as \textbf{column store}.
                            An example of this implementation is as follows;
                            \begin{lstlisting}
                                struct DSMTable : public Table {
                                    using Column = vector<AttributeValue>;
                                    vector<Column> data;
                                    vector<bool> deleteMarkers;

                                    void insert(Tuple);
                                    vector<Tuple> findTuplesWithAttributeValue(int attributePosition, AttributeValue value) {};
                                    void deleteTuplesWithAttributeValue(int attributePosition, AttributeValue value) {};
                                };

                                // inserts cause tuple decomposition
                                void DSMTable::insert(Tuple tuple) {
                                    for (int i = 0; i < tuple.size(); i++) {
                                        data[i].push_back(tuple[i]);
                                    }
                                }

                                // find requires tuple reconstruction
                                vector<Table::Tuple> DSMTable::findTuplesWithAttributeValue(int attributePosition, AttributeValue value) {
                                    vector<Tuple> result;
                                    for (size_t i = 0; i < data[attributePosition].size(); i++) {
                                        if (data[attributePosition][i] == value && !deleteMarkers[i]) {
                                            Tuple reconstructedTuple;
                                            for (int column = 0; column < data.size(); column++) {
                                                reconstructedTuple.push_back(data[column][i]);
                                            }
                                            result.push_back(reconstructedTuple);
                                        }
                                    }
                                    return result;
                                }

                                void DSMTable::deleteTuplesWithAttributeValue(int attributePosition, AttributeValue value) {
                                    for (size_t i = 0; i < data.size(); i++) {
                                        if (data[attributePosition][i] == value) {
                                            deleteMarkers[i] = true;
                                        }
                                    }
                                }
                            \end{lstlisting}
                            Decomposed storage works well in the case when we are iterating over one column of a tuple.
                            However, if these entries are one after the other, there is perfect data locality, which minimises the number of cache lines we need.
                            On the other hand, insertion is suboptimal as we need to spread a tuple over memory.
                            Similarly, accessing a single tuple, even when we know where it is in memory is suboptimal, as the tuple will need to be reconstructed.
                    \item Hybrid Delta / Main Storage
                        \begin{lstlisting}
                            struct HybridTable : public Table {
                                DSMTable main; // every tuple will eventually end up here
                                NSMTable delta; // will be inserted here then merged into main

                                void insert(Tuple);
                                vector<Tuple> findTuplesWithAttributeValue(int attributePosition, AttributeValue value) {};
                                void deleteTuplesWithAttributeValue(int attributePosition, AttributeValue value) {};
                                void merge();
                            };

                            void HybridTable::insert(Tuple t) {
                                delta.insert(t);
                            }

                            vector<Table::Tuple> HybridTable::findTuplesWithAttributeValue(int attributePosition, AttributeValue value) {
                                vector<Tuple> results = main.findTuplesWithAttributeValue(attributePosition, value);
                                vector<Tuple> fromDelta = delta.findTuplesWithAttributeValue(attributePosition, value);
                                results.insert(results.end(), fromDelta.begin(), fromDelta.end());
                                return results;
                            }

                            void HybridTable::deleteTuplesWithAttributeValue(int attributePosition, AttributeValue value) {
                                main.deleteTuplesWithAttributeValue(attributePosition, value);
                                delta.deleteTuplesWithAttributeValue(attributePosition, value);
                            }

                            void HybridTable::merge() {
                                for (auto i = 0u; i < delta.data.size(); i++) {
                                    // these two operations need to atomic (otherwise we can return the same tuple multiple times)
                                    main.insert(delta.data[i].actualTuple);
                                    delta.data[i].deleted = true;
                                }
                            }
                        \end{lstlisting}
                \end{itemize}
                In conclusion, \textbf{DSM} works well fro scan-heavy queries (accessing a lot of tuples but few columns).
                This is common in analytical processing, and analytics mostly operate on historical data.
                On the other hand \textbf{NSM} works well for lookups and inserts.
                This is more common in transactional processing (inserting sales item, looking up products, etc).
                Transactions mostly operate on recent data.
                Hybrid exploits the aforementioned workloads, but it needs regular migrations (~merge()~) which may need to lock the database.
            \subsubsection*{Catalogue}
                The catalogue stores metadata.
                The idea is that real-life data follows patterns, which if recognised and exploited, can lead to more efficiency.
                However metadata will need to be stored and maintained.
                Examples of metadata can be type, min / max values (if data is requested outside a range we know it doesn't exist), histograms, etc.
                Two simple ones are \textbf{sortedness} and \textbf{denseness}.
                Consider the following example, which is a common pattern (as the first column is often IDs);
                \begin{lstlisting}
                    class Table {
                        vector<Tuple> storage;
                        bool firstColumnIsSorted = true;
                        bool firstColumnIsDense = true;

                        public:
                            void insert(Tuple t) {
                                if (storage.size() > 0) {
                                    firstColumnIsSorted &= (t[0] >= storage.back()[0]);
                                    firstColumnIsDense &= (t[0] == storage.back()[0] + 1);
                                }
                                storage.push_back(t);
                            }

                            vector<Tuple> findTuplesWithAttributeValue(int attribute, AttributeValue value) {
                                if (attribute == 0 && firstColumnIsDense) {
                                    return { data[value - storage.front()[0]] };
                                } else if (attribute == 0 && firstColumnIsSorted) {
                                    if (binary_search(data, attribute, value)[0] == value) {
                                        return { binary_search(data, attribute, value) };
                                    }
                                }
                                ... // same scan as before
                            }

                            // we can also exploit the fact that order of rows doesn't matter, and therefore we can reorganise
                            void analyse() {
                                // this sort can be expensive (and therefore should be run regularly but not always)
                                sort(storage.begin(), storage.end(), [](auto l, auto r) { return l[0] < r[0] });
                                firstColumnIsSorted = true;
                                firstColumnIsDense = true;
                                for (size_t = 1; i < storage.size(); i++) {
                                    firstColumnIsDense &= storage[i][0] == storage[i - 1][0] + 1;
                                }
                            }
                    }
                \end{lstlisting}
            \subsubsection*{Variable Length Data}
                It's important to note that strings tend to have different lengths.
                However, we'd like to maintain fixed tuple sizes, as it allows for random access to tuples by their position.
                We can either overallocate space for ~varchars~ (for example, some system require a size parameter for maximum length), or we can store them out of place.
                Overallocating leads to strings under the maximum length being padded with some sort of terminator.
                This is good for locality and is simple to implement, however it's very wasteful for space (especially if they are too generous with lengths).
                \medskip

                On the other hand, out of place storage now contain an index into a dictionary, which contains the string (all null terminated).
                However, retrieving a value will need an access to obtain the index and another to get the actual value (which has poor locality).
                While this is better for space (and is what most programming languages do), it's also complicated (and causes difficult garbage-collection).
                This gives an optimisation (\textbf{dictionary compression}) however - if we find a string that has already been used when we insert, we can use the address of the existing value for the insert.
            \subsubsection*{Data Storage on Disk}
                Disks are different from main memory in a number of ways;
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{larger pages} \hfill kilobytes compared to bytes
                    \item \textbf{higher latency} \hfill milliseconds compared to nanoseconds
                    \item \textbf{lower throughput} \hfill hundreds of megabytes instead of tens of gigabytes per second
                        \subitem this is why we consider DBMSs as I/O bound
                    \item \textbf{OS gets in the way} \hfill filesize can be limited, therefore DMBS need to map files and offsets
                \end{itemize}
                Our goals also change slightly; disks now dominate cost, and therefore complicated I/O management strategies can pay off.
                Due to larger pages, each page behaves like a mini-database in the case of N-ary storage.
                The basic kernel now looks like the following (where the catalogue is in memory, but the data is in disk);
                \begin{center}
                    \begin{tikzpicture}[x=1.5cm, y=0.5cm]
                        \draw
                        (0, 0) -- (9, 0) -- (9, -5) -- (0, -5) -- cycle
                        (1, -1) -- (4, -1) -- (4, -2) -- (1, -2) -- cycle
                        (5, -1) -- (8, -1) -- (8, -2) -- (5, -2) -- cycle
                        (1, -3) -- (4, -3) -- (4, -4) -- (1, -4) -- cycle
                        (5, -1) -- (8, -1) -- (8, -2) -- (5, -2) -- cycle
                        (-3, 0) -- (-1, 0) -- (-1, -2) -- (-3, -2) -- cycle
                        (-3, -3) -- (-1, -3) -- (-1, -5) -- (-3, -5) -- cycle;

                        \node at (2.5, -1.5) {storage manager};
                        \node at (2.5, -3.5) {buffer manager};
                        \node at (6.5, -1.5) {op. implementations};
                        \node at (-2, -1) {catalogue};
                        \node at (-2, -4) {data};
                        \node at (6.5, -4) {database kernel};
                        \draw
                        (6, -2) edge[->] (3, -3)
                        (2.5, -2) edge[->] (2.5, -3)
                        (1, -1.5) edge[->] (-1, -1.5)
                        (1, -3.5) edge[->] (-1, -3.5);
                    \end{tikzpicture}
                \end{center}
                The \textbf{buffer manager} manages disk-resident data.
                It maps unstructured files (large 'blobs of bytes') to structured tables for reading and writing.
                Since DBs don't trust OSs, it makes sure files have a fixed size, and it also safely writes data to disk when necessary.
                It also writes in \textbf{open (for writing) pages}.
                An example implementation is as follows;
                \begin{lstlisting}
                    class BufferManager;
                    class Table {
                        BufferManager& bufferManager;
                        string relationName = "Employee";

                        public
                            void insert(Tuple t) {
                                bufferManager.getOpenPageForRelation(relationName).push_back(t);
                                bufferManager.commitOpenPageForRelation(relationName);
                            }

                            vector<Tuple> findTuplesWithAttributeValue(int attribute, AttributeValue value) {
                                vector<Tuple> result;
                                auto pages = bufferManager.getPagesForRelation(relationName);
                                for (size_t i = 0; i < pages.size(); i++) {
                                    auto page = pages[i];
                                    for (size_t i = 0; i < page.size(); i++) {
                                        if (page[i][attribute] == value) {
                                            result.push_back(page[i]);
                                        }
                                    }
                                }
                                return result;
                            }
                    }

                    struct BufferManager {
                        using Tuple = vector<AttributeValue>;
                        using Page = vector<Tuple>;

                        size_t tupleSize;
                        map<string, vector<Tuple>> openPages;
                        map<string, vector<string>> pagesOnDisk; // maps one relation to many pages on disk (filenames)
                        size_t numberOfTuplesPerPage();

                        vector<Tuple>& getOpenPageForRelation(string relationName);
                        void commitOpenPageForRelation(string relationName);
                        vector<Page> getPagesForRelation(string, relationName);
                    }

                    vector<Tuple>& BufferManager::getOpenPageForRelation(string relationName) {
                        return openPages[relationName]; // creates one if needed
                    }

                    void BufferManager::commitOpenPageForRelation(string relationName) {
                        while (openPages[relationName].size() >= numberOfTuplesPerPage(tupleSize)) {
                            vector<Tuple> newPage;

                            // move overflowing tuples to new page
                            while (openPages[relationName].size() > numberOfTuplesPerPage(tupleSize)) {
                                newPage.push_back(openPages[relationName].back());
                                openPages[relationName].pop_back();
                            }

                            pagesOnDisk[relationName].push_back(writeToDisk(openPages[relationName]));
                            openPages[relationName] = newPage; // contains tuples that didn't fit on disk
                        }
                    }

                    vector<vector<Tuple>> BufferManager::getPagesForRelation(string relationName) {
                        vector<vector<Tuple>> result = { openPages[relationName] };
                        for (size_t i = 0; i < pagesOnDisk[relationName].size(); i++) {
                            result.push_back(readFromDisk(pagesOnDisk[relationName][i]));
                        }
                        return result;
                    }
                \end{lstlisting}
                However, we still need to determine ~numberOfTuplesPerPage~;
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{unspanned pages} \hfill pages like mini-databases
                        \subitem
                            The goal of this is simplicity, and good random access performance; we want to find the record with a single page lookup, given a ~tuple\_id~.
                            When we don't have enough space on a page for another tuple, we will write to disk (even when not full) and obtain a new page.
                            However, we can quite easily infer which page a tuple will be on, since we know how many tuples are on each page.
                            This cannot deal with large records (where the size is larger than a page), nor can we have in-page random access if the records are variable size.
                            \begin{lstlisting}
                                const long pageSizeInBytes = 4096;
                                size_t BufferManager::numberOfTuplesPerPage() {
                                    return floor(pageSizeInBytes / tupleSizeInBytes);
                                }

                                // space consumption of a relation of n tuples;
                                ceil(data.size() / mnumberOfTuplesPerPage())
                            \end{lstlisting}
                    \item \textbf{spanned pages} \hfill optimising for space efficiency
                        \subitem
                            On the other hand, the goal here is to minimise space waste and to support large records.
                            Spanned pages can have tuples existing across pages - this is complicated and also hurts random access performance, since we can no longer easily determine where a tuple is.
                            We can calculate the number of pages per relation as follows;
                            \begin{lstlisting}
                                long dataSizeInBytes = tupleSizeInBytes * data.size();
                                long numberOfPagesForTable = ceil(dataSizeInBytes / pageSizeInBytes);
                            \end{lstlisting}
                            However - we cannot determine the number of tuples per page as this is not constant.
                    \item \textbf{slotted pages} \hfill random access for in-place NSM
                        \subitem
                            This is the most complicated method covered, but is also what is used by most systems.
                            This stores tuples in in-place N-ary format, and stores the tuple count in the \textbf{page header} (at the start of the page).
                            Offsets are also stored to every tuple, which are filled in from the \textbf{end} of the page (hence the index of the first tuple is the last item in the page and so on).
                            Offsets need to be typed large enough to address page;
                            \begin{itemize}
                                \itemsep0em
                                \item ~byte~s for pages smaller than 256 bytes
                                \item ~short~s for pages smaller than 65,536 bytes
                                \item ~int~s for pages smaller than 4 gigabytes
                            \end{itemize}
                \end{itemize}
                Some disk-based database systems also keep a dictionary per page, which solves the problem of variable sized records (and allows for duplicate elimination, at the granularity of a single page, as previously mentioned).
                A global dictionary is not used as it will lead to more accesses.
        \subsection*{Lecture 3}
            \subsubsection*{Join}
                The issue with data normalisation (see \textbf{CO130}) is that data ends up scattered across different tables.
                For example, consider the following example tables.
                Notice how a simple application is now split across these 4 tables - but we care more about who ordered what book, rather than the order IDs.
                \begin{center}
                    \hfill
                    \begin{tabular}{ccc}
                        \multicolumn{3}{c}{~Customer~} \\
                        ~ID~ & ~Name~ & ~City~ \\
                        \hline
                        ~1~ & ~james~ & ~London~ \\
                        ~2~ & ~steve~ & ~London~ \\
                        ~3~ & ~kate~ & ~Manchester~ \\
                        \phantom{}
                    \end{tabular}
                    \hfill
                    \begin{tabular}{cc}
                        \multicolumn{2}{c}{~Order~} \\
                        ~ID~ & ~CustomerID~ \\
                        \hline
                        ~1~ & ~1~ \\
                        ~2~ & ~2~ \\
                        ~3~ & ~3~ \\
                        \phantom{}
                    \end{tabular}
                    \hfill
                    \begin{tabular}{cc}
                        \multicolumn{2}{c}{~OrderedItem~} \\
                        ~ID~ & ~CustomerID~ \\
                        \hline
                        ~1~ & ~1~ \\
                        ~1~ & ~2~ \\
                        ~2~ & ~1~ \\
                        ~3~ & ~3~
                    \end{tabular}
                    \hfill
                    \phantom{}
                    \smallskip

                    \hfill
                    \begin{tabular}{ccc}
                        \multicolumn{3}{c}{~Book~} \\
                        ~ID~ & ~Title~ & ~Author~ \\
                        \hline
                        ~1~ & ~Fahrenheit 451~ & ~Ray Bradbury~ \\
                        ~2~ & ~Animal Farm~ & ~George Orwell~ \\
                        ~3~ & ~Distributed Systems~ & ~van Steen \& Tanenbaum~ \\
                        \phantom{}
                    \end{tabular}
                    \hfill
                    \phantom{}
                \end{center}
                Joins are very common not only due to normalisation (with mostly \textbf{Foreign-Key joins}), but also due to the value produced by combining data.
                For example, we can find users that purchase the same product with joins, or finding what advertisements work (seeing which users search for a term within a timeframe after seeing an advert).
                \medskip

                Joins are basically cross products with a selection \textbf{involving both inputs}.
                The variations on joins are based on how tuples without corresponding rows on either input are matched;
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{left join} \hfill ($R \ljoin S$)
                        \subitem
                            This returns all rows in $R$, even if no rows in $S$ match (in this case it fills the columns of $S$ with ~NULL~ values)
                    \item \textbf{right join} \hfill ($R \rjoin S$)
                        \subitem
                            Same as above, but the inverse
                    \item \textbf{full outer join} \hfill ($R \ojoin S$)
                        \subitem
                            Returns every row in $R$ as well as every row in $S$ (even if no rows are matching) - similarly will fill with ~NULL~ values if nothing is matching;
                            $$R \ojoin S \equiv (R \ljoin S) \cup (R \rjoin S)$$
                \end{itemize}
                An example of the matching function is as follows;
                \begin{center}
                    ~SELECT * FROM R JOIN S ON (R.r = S.s)~
                \end{center}
                The matching function for joins does not have to be equality;
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{equi-join} (algorithmically equivalent to intersections) \hfill equality
                    \item \textbf{inequality joins} \hfill inequality constraint (~<~ or ~>~)
                    \item \textbf{anti-join} \hfill ~<>~ or ~!=~
                    \item \textbf{Theta joins} \hfill all other joins (difficult to optimise)
                \end{itemize}
            \subsubsection*{Join Algorithms}
                Some of the join algorithms are as follows;
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{nested loop join}
                        \begin{lstlisting}
                            using Table = vector<vector<int>>;
                            Table left, right;
                            for (size_t i = 0; i < leftRelationSize; i++) {
                                auto leftInput = left[i];
                                for (size_t j = 0; j < rightRelationSize; j++) {
                                    auto rightInput = right[j];
                                    if (leftInput[leftAttribute] == rightInput[rightAttribute]) {
                                        writeToOutput({leftInput, rightInput});
                                    }
                                }
                            }
                        \end{lstlisting}
                        This is a simple algorithm and is trivial to parallelise (since there are no dependent loop iterations).
                        This also has sequential I/O, which is good for the buffer manager.
                        The effort required however is;
                        $$\Theta(|L| \times |R|) \text{ , reduced to } \Theta\left(\frac{|L| \times |R|}{2}\right) \text{ if uniqueness can be assumed}$$
                    \item \textbf{sort-merge joins}
                        \begin{lstlisting}
                            // this assumes values are unique and sorted
                            auto leftI = 0;
                            auto rightI = 0;
                            // keep iterating until either input is finished
                            while (leftI < leftInputSize && rightI < rightInputSize) {
                                auto leftInput = left[leftI];
                                auto rightInput = right[leftI];
                                if (leftInput[leftAttribute] < rightInput[rightAttribute]) {
                                    leftI++;
                                } else if (rightInput[rightAttribute] > leftInput[leftAttribute]) {
                                    rightI++;
                                } else {
                                    writeToOutput({leftInput, rightInput});
                                    rightI++;
                                    leftI++;
                                }
                            }
                        \end{lstlisting}
                        The idea of this is that we first sort both inputs and have cursors starting at the start of each input.
                        If the value at the left pointer is smaller than the value at the right pointer, we increment the left pointer (and vice versa).
                        If it's the same, then we can output a match.
                        \medskip

                        Assuming (without loss of generality) the value of on the left is less than the value on the right.
                        All values after the value on the right are greater than it (due to the sorted nature).
                        Therefore no value after the value on the right can be a join partner to the value on the left, therefore we can advance the cursor on the left.
                        \begin{center}
                            \begin{tikzpicture}[x=1.5cm, y=0.75cm]
                                \node (f) at (0, -3) {5};
                                \node (s) at (3, -2) {7};
                                \draw
                                (0, 0) edge[dashed] (f)
                                (f) edge[dashed, left] node{$>5$} (0, -5)
                                (3, 0) edge[dashed] (s)
                                (s) edge[dashed, right] node{$>7$} (3, -5)
                                (5, 0) edge[->, right] node{merge direction} (5, -5)
                                (f) edge[<->, above] node{comparison} (s);
                            \end{tikzpicture}
                        \end{center}
                        The effort required for this is as follows (the merge must go through each tuple on either side);
                        $$O(\underbrace{|L| \times \log|L|}_{O(~sort~(L))} + \underbrace{|R| + \log|R|}_{O(~sort~(R))} + \underbrace{|L| + |R|}_{O(~merge~)})$$
                        This has sequential I/O in the merge phase, however it is tricky to parallelise.
                        This also works for inequality joins, but we need to be more careful when advancing the cursors.
                    \item \textbf{hash joins}
                        \begin{lstlisting}
                            vector<optional<vector<int>>> hashTable; // optional since slots can be empty
                            int hash(int);
                            int nextSlot(int);

                            for (size_t i = 0; i < buildSide.size(); i++) {
                                auto buildInput = buildSide[i];
                                auto hashValue = hash(buildInput[buildAttribute]);

                                // avoid overwriting a value
                                while (hashTable[hashValue].hasValue) {
                                    hashValue = nextSlot(hashValue);
                                }
                                hashTable[hashValue] = buildInput;
                            }

                            for (size_t i = 0; i < probeSide.size(); i++) {
                                auto probeInput = probeSide[i];
                                auto hashValue = hash(probeInput[probeAttribute]);
                                while (hashTable[hashValue].hasValue && hashTable[hashValue].value[buildAttribute] != probeInput[probeAttribute]) {
                                    hashValue = nextSlot(hashValue);
                                }
                                if (hashTable[hashValue].value[buildAttribute] == probeInput[probeAttribute]) {
                                    writeToOutput({hashTable[hashValue].value, probeInput})
                                }
                            }
                        \end{lstlisting}
                        We need to first distinguish the \textbf{build-side} (side buffered in the hashtable) and \textbf{probe-side} (side used to look up tuples in the hashtable).
                        \medskip

                        For this, we need to establish some requirements on the hash function.
                        The requirements is that it is pure (and has no state), and need to know the output domain (the range of generated values) in order to allocate the size.
                        It's also ideal to have a contiguous output domain (without holes) and a uniform distribution (where all values are equally likely for consistent performance).
                        Some common examples are;
                        \begin{itemize}
                            \itemsep0em
                            \item \textbf{MD5}
                            \item \textbf{Modulo-Division} \hfill simplest (but input skew leads to output skew)
                            \item \textbf{MurmurHash} \hfill one of fastest, decent hash-functions
                            \item \textbf{CRC32} \hfill also has input skew issue, but has hardware support
                        \end{itemize}
                        Furthermore, we need to handle conflicts on hash collision.
                        This needs to have some locality (if there is too much locality, there can be issues when inserting a lot of data into the same slot).
                        Additionally, it needs to have no holes (we want to probe all slots to avoid memory waste).
                        Some approaches are as follows;
                        \begin{itemize}
                            \itemsep0em
                            \item \textbf{linear probing}
                                \subitem
                                    When a slot is filled, try the next one and continue doing so until a free slot is found - wrapping around to the start at the end of the buffer.
                                    This approach is simple and has great access locality.
                                    However it can lead to long probe-chains for adversarial input data (if we have input locality).
                            \item \textbf{quadratic probing}
                                \subitem
                                    This is similar to the above strategy, but we instead double the distance from the initial slot each time, first checking one that is a distance of 1 away, then 2, then 4, and so on, also wrapping at the end of the buffer.
                                    This is also simple, but only has good locality for the first few probes (and gets much worse).
                                    The first few probes are also likely to cause conflicts.
                            \item \textbf{rehashing}
                                \subitem
                                    We want to distribute probes uniformly, which can cause poor access locality but reduces conflicts.
                                    To do so, we can just use the hash function again.
                                    However, to ensure all slots are probed, we need to consider \textbf{cyclic groups}.
                        \end{itemize}
                        An example of this applied, with \textbf{modulo hashing} (with a constant factor of 10 for simplicity) and \textbf{linear probing} is as follows;
                        \begin{lstlisting}
                            int hash(int v) { return v % 10; }
                            int probe(int v) { return (v + 1) % 10; }

                            buildSide = {1, 2, 7, 8, 12, 16, 17};
                        \end{lstlisting}
                        With this, we'd end up with the following slots in the hash table (~-~ denotes unused);
                        \begin{center}
                            ~(index 0) => [-, 1, 2, 12, -, -, 16, 7, 8, 17] <= (index 9)~
                        \end{center}
                        Hash joins give us sequential I/O on inputs but pseudo-random access to the hash-table during build and probe.
                        It's parallelisable over the values on the probe side, but parallelising the build is difficult.
                        The effort required is;
                        $$\Theta(|~build~| + |~probe~|) \text{ (best case) and } O(|~build~| \times |~probe~|) \text{ (worst case)}$$
                        However, it's also important to consider that we typically want to store more than a single value, and often we may not have the uniqueness guarantee.
                        Good hashing is still expensive and requires lots of CPU cycles (more expensive than multiple data accesses).
                        Slots are often also allocated in buckets, allow for more than one tuple per slot.
                        It's roughly equivalent to rounding every hash value down to a multiple of the bucket size.
                        Sometimes buckets are implemented as linked lists (\textbf{bucket-chaining}, \textbf{open addressing}) which is a bad idea for lookup performance.
                        Hashtables are also arrays, which occupy space (typically overallocated by a factor of two to reduce long probe chains).
                        They are also probed randomly in the probe phase, ideally we want to keep the hash table in memory / cache, but not to disk.
                        \textbf{For this class}, if the hashtable doesn't fit, \textbf{every access has a constant penalty}.
                        Generally we use hash joins when one relation is much smaller than the other.
                \end{itemize}
            \subsubsection*{Partitioning}
                We have the fundamental premise that sequential access is much cheaper than random access, and the difference grows with the page size.
                If we assume a random value access cost $c$, the sequential value access cost is;
                $$\frac{c}{\text{pagesize}_\text{OS}}$$
                Assuming the hashtable doesn't fit in the page / cache, we will use the cost $c$ quite frequently, and therefore the extra pass for partitioning, which is mostly sequential access might be worth doing.
                We can visualise thrashing as follows;
                \begin{center}
                    \begin{tikzpicture}
                        \node at (1, 0.5) {build};
                        \node at (5, 0.5) {hashtable};
                        \node at (11, 0.5) {probe};
                        \node at (8, 0.5) {\shortstack{random access to\\large array, bad\\cache locality}};
                        \draw
                        (2, -3) edge[->] (4, -3)
                        (0, 0) -- (2, 0) -- (2, -6) -- (0, -6) -- cycle
                        (4, 0) -- (6, 0) -- (6, -6) -- (4, -6) -- cycle
                        (10, 0) -- (12, 0) -- (12, -6) -- (10, -6) -- cycle;
                        \draw
                        (10, -1) edge[->] (6, -1)
                        (10, -2) edge[->] (6, -5)
                        (10, -3) edge[->] (6, -4)
                        (10, -4) edge[->] (6, -2)
                        (10, -5) edge[->] (6, -3);
                    \end{tikzpicture}
                \end{center}
                On the other hand, by using a partitioning function (which typically has a small range to fit stuff into memory);
                \begin{center}
                    \begin{tikzpicture}
                        \node at (1, 0.5) {input};
                        \node at (4, 0.5) {\shortstack{partitioning\\function (~\% 4~)}};
                        \node at (8, 0.5) {\shortstack{memory\\(5 pages)}};
                        \node at (13, 0.5) {\shortstack{pages\\on disk}};
                        \node at (1, -0.5) {14};
                        \node at (1, -1.5) {12};
                        \node at (1, -2.5) {17};
                        \node at (1, -3.5) {5};
                        \node (p0) at (4, -0.5) {2};
                        \node (p1) at (4, -1.5) {0};
                        \node (p2) at (4, -2.5) {1};
                        \node (p3) at (4, -3.5) {1};
                        \node at (13, -0.5) {hash 0};
                        \node at (13, -4) {hash 1};
                        \node at (13, -6.5) {hash 2};
                        \node at (1, -4.5) {$\vdots$};
                        \draw
                        (2, -0.5) edge[dashed, ->] (p0)
                        (p0) edge[->] (7, -2.5)
                        (p1) edge[->] (7, -0.5)
                        (p2) edge[->] (7, -1.5)
                        (p3) edge[->] (7, -1.5)
                        (2, -1.5) edge[dashed, ->] (p1)
                        (2, -2.5) edge[dashed, ->] (p2)
                        (2, -3.5) edge[dashed, ->] (p3)
                        (9, -0.5) edge[->] (12, -0.5)
                        (9, -1.5) edge[->] (12, -4)
                        (9, -2.5) edge[->] (12, -6.5);
                        \draw
                        (0, -1) -- (2, -1)
                        (0, -2) -- (2, -2)
                        (0, -3) -- (2, -3)
                        (0, 0) -- (2, 0) -- (2, -4) -- (0, -4) -- cycle;
                        \draw
                        (7, -1) -- (9, -1)
                        (7, -2) -- (9, -2)
                        (7, -3) -- (9, -3)
                        (7, -4) -- (9, -4)
                        (7, 0) -- (9, 0) -- (9, -5) -- (7, -5) -- cycle;
                        \draw
                        (12, 0) -- (14, 0) -- (14, -3) -- (12, -3) -- cycle
                        (12, -3.5) -- (14, -3.5) -- (14, -5.5) -- (12, -5.5) -- cycle
                        (12, -6) -- (14, -6) -- (14, -8) -- (12, -8) -- cycle;
                    \end{tikzpicture}
                \end{center}
                The write from input to memory is done for every tuple, but the write from memory to the pages on the disk is only done when the page is \textbf{filled}, which doesn't waste bandwidth (and also lowers the number of $c$s we use).
                As such, the probing becomes more involved, we first partition the probe in the same way, and we then only look at the partitioned hashtables, which gives us localised random access.
                \medskip

                Another benefit of partitioning is that we can now parallelise the processing of each of the smaller joins, since we know they are disjoint.
            \subsubsection*{Indexing}
                We should note that these algorithms are done in phases (build / probe and sort / merge).
                Typically the first phase, build or sort, is independent of the query phase, and can be done before any data is requested (such as when it is first loaded in).
                This is called indexing.
                \medskip

                An index is a secondary storage, which is about replicating data.
                The replication is controlled by the DBMS, which means they can be created and destroyed without breaking the system.
                These replicas are semantically invisible to the user (doesn't change results).
                On the other hand, these replicas occupy space and need to be maintained under updates.
                \medskip

                A primary index is used to store the tuples of a table, not a table, and therefore only one of these can be used per table.
                On the other hand, a secondary index stores pointers to the tuple of a table, hence we can have many of these in a table.
                \medskip

                In SQL, an index can be created and destroyed as follows (however, it's unclear what type of index is created, nor do we have control over parameters);
                \begin{lstlisting}
                    CREATE INDEX index_name ON table_name (column1, column2, ...);
                    DROP INDEX index_name;
                \end{lstlisting}
                Some examples are as follows;
                \begin{itemize}
                    \itemsep0em
                    \item \textbf{Hash-Indexing}
                        \subitem
                            The first step of a hash-join was building a hash-table, instead the hash-index is the same but persistent.
                            In an unclustered hash-index, there is hash table which stores the key and position, and in the relation it is ordered by the position (and contains the actual data).
                            \medskip

                            The \textbf{ephemeral} hashtables we built for hash-joins were temporary, and we assume that no new tuples are added during the evaluation of the query, and we'd also know a rough amount of tuples that would end up in the table.
                            However, all of this changes if the hash-table is persistent.
                            Since persistent tables may grow arbitrarily large, we need to overallocate by a lot.
                            If the fill-factor grows beyond $x$ percent, we need to rebuild it with a larger overallocation - this can be expensive (and leads to load spikes on the inserts causing rebuilds).
                            Similar, deletion can also be problematic.
                            \medskip

                            Previously, we used empty slots to mark an end of a probe-chain.
                            However, a value must remain if something is deleted - we can either leave the value and mark it as deleted, or put the last value in the probe chain in the position.
                            A deletion strategy (to delete key ~k~) is as follows;
                            \begin{itemize}
                                \itemsep0em
                                \item hash ~k~, find ~k~, and keep a pointer to ~k~
                                \item continue probing until the end of the probe chain is found
                                \item if the value at the end of the chain has the same hash as ~k~, move it into ~k~'s slot
                                \item otherwise mark ~k~ as deleted (we can then fill this slot with the next value that hashes into the probe chain)
                            \end{itemize}
                            An example is as follows.
                            Note that 14 has the same hash as 23 (under modulo 9), but 14 has a different hash to 17.
                            \begin{center}
                                \begin{tabular}{c|ccc|c}
                                    \multicolumn{5}{c}{before} \\
                                    \hline
                                    key & \multicolumn{3}{c|}{non-key} & deleted \\
                                    \hline
                                    9 & 16 & 9 & 34 & \\
                                    27 & 5 & 61 & 45 & \\
                                    12 & 12 & 78 & 1 & \\
                                    & & & & \\
                                    23 & 84 & 17 & 69 & \\
                                    5 & 45 & 71 & 20 & \\
                                    17 & 9 & 42 & 83 & \\
                                    14 & 21 & 55 & 2 & \\
                                    & & & &
                                \end{tabular}
                                \hfill
                                \begin{tabular}{c|ccc|c}
                                    \multicolumn{5}{c}{after delete 23} \\
                                    \hline
                                    key & \multicolumn{3}{c|}{non-key} & deleted \\
                                    \hline
                                    9 & 16 & 9 & 34 & \\
                                    27 & 5 & 61 & 45 & \\
                                    12 & 12 & 78 & 1 & \\
                                    & & & & \\
                                    14 & 21 & 55 & 2 & \\
                                    5 & 45 & 71 & 20 & \\
                                    17 & 9 & 42 & 83 & \\
                                    & & & & \\
                                    & & & &
                                \end{tabular}
                                \hfill
                                \begin{tabular}{c|ccc|c}
                                    \multicolumn{5}{c}{after delete 14} \\
                                    \hline
                                    key & \multicolumn{3}{c|}{non-key} & deleted \\
                                    \hline
                                    9 & 16 & 9 & 34 & \\
                                    27 & 5 & 61 & 45 & \\
                                    12 & 12 & 78 & 1 & \\
                                    & & & & \\
                                    14 & 21 & 55 & 2 & $\times$ \\
                                    5 & 45 & 71 & 20 & \\
                                    17 & 9 & 42 & 83 & \\
                                    & & & & \\
                                    & & & &
                                \end{tabular}
                            \end{center}
                            Similar to hash-joins, persistent hash-tables are good for hash-joins and aggregations (assuming they are built on the join / aggregation key columns).
                            They also reduce the number of candidates for equality selections, but don't help on much else.
                    \item \textbf{Bitmap-Index}
                        \subitem
                            It's first important to establish a \textbf{bitvector} is a sequence of 1-bit values indicating a boolean condition holding for the elements of a sequence of values.
                            However, it's important to note that CPUs don't work with individual bits but rather words (let us assume 8 bit words, although in reality it's at least 32 bits);
                            \begin{align*}
                                BV_{== 7}([4, 7, 11, 7, 7, 11, 4, 7]) & = [0, 1, 0, 1, 1, 0, 0, 1] \\
                                & = 128 \cdot 0 + 64 \cdot 1 + 32 \cdot 0 + 16 \cdot 1 + 8 \cdot 1 + 4 \cdot 0 + 2 \cdot 0 + 1 \cdot 1 \\
                                & = 89
                            \end{align*}
                            Bitmap indices are a collection of bitvectors on a column (one for each distinct value in that column).
                            This is useful if there are few distinct values and these bitvectors are disjoint.
                            \begin{center}
                                \begin{tabular}{c|ccc}
                                    column & \multicolumn{3}{c}{bitmap index} \\
                                    & ~==5~ & ~==3~ & ~==9~ \\
                                    \hline
                                    5 & 1 & 0 & 0 \\
                                    3 & 0 & 1 & 0 \\
                                    9 & 0 & 0 & 1 \\
                                    9 & 0 & 0 & 1 \\
                                    9 & 0 & 0 & 1 \\
                                    5 & 1 & 0 & 0 \\
                                    3 & 0 & 1 & 0 \\
                                    9 & 0 & 0 & 1
                                \end{tabular}
                            \end{center}
                            An implementation of this is as follows;
                            \begin{lstlisting}
                                unsigned char** bitmaps;
                                void scanBitmap(byte* column, size_t inputSize, byte value) {

                                    // finds specific bitmap for a certain value
                                    unsigned char* scannedBitmap = bitmapForValue(bitmaps, value);

                                    // iterate over bytes in bitmap
                                    for (size_t i = 0; i < inputSize / 8; i++) {
                                        // skip if none of the values are set
                                        if (scannedBitmap[i] != 0) {
                                            unsigned char bitmapMask = 128; // binary 10000000
                                            for (size_t j = 0; j < 8; j++) {
                                                if ((bitmapMask & scannedBitmap[i]) && column[i * 8 + j] == value) {
                                                    writeOutput(column[i * 8 + j]);
                                                }
                                                bitmapMask >>= 1;
                                            }
                                        }
                                    }
                                }
                            \end{lstlisting}
                            This is useful as it reduces the bandwidth need for scanning a column, in the order of the size of the type of the column in bits (instead of loading a 32 bit value, we can just check 1 bit).
                            Predicates can now also be combined using logical operators on the bitvectors.
                            Additionally, arbitrary boolean conditions (such as ranges) can be indexed by some systems.
                            \medskip

                            Binned bitmaps uses the idea of having $n$ bitvectors, each with a predicate covering a different part of the value domain.
                            For example if we assume our column type is a ~byte~, we can have the following three bins;
                            \begin{itemize}
                                \itemsep0em
                                \item bin 1: 0 - 7
                                \item bin 2: 8 - 20
                                \item bin 3: 21 - 255
                            \end{itemize}
                            We need to ensure the conditions span the entire value domain.
                            Also, the index cannot distinguish values in a bin (unless it only contains one value) - it can only produce \textbf{candidates}, some of which may be false positives.
                            \medskip

                            Generally there are two strategies for binning;
                            \begin{itemize}
                                \itemsep0em
                                \item \textbf{equi-width}
                                    \subitem
                                        This is simple to configure - we can calculate our bin-width to be
                                        $$\frac{\max(~column~) - \min(~column~)}{~numberOfBins~}$$
                                        This has limited use when indexing non-uniformly distributed data (since there can be a high frequency of false positives in a highly populated bin).
                                \item \textbf{equi-height}
                                    \subitem
                                        This method is more resilient against non-uniformly distributed data - the false positive rate is value independent.
                                        We want to have the same number of counts per bin.
                                        The construction is more difficult however, as we need to determine quantiles and is usually performed on samples.
                                        If the distribution changes, it will need to be rebinned (which can be expensive).
                            \end{itemize}
                            Additionally, when we are binning, we may encounter many consecutive values which are equal.
                            Each of these consecutive values can be replaced with the value (the Run) as well as the number of tuples (length) - this works very well on high-locality data.
                            For example, we can transform ~[0, 1, 1, 1, 1, 1, 0, 1]~ to ~[(0, 1), (1, 5), (0, 1), (1, 1)]~.
                            However, this will now require a sequential scan, which can be fixed with \textbf{length prefix summing} (which stores the start of the run instead of the length);
                            ~[(0, 3), (1, 3), (0, 1), (1, 1)]~ to ~[(0, 0), (1, 3), (0, 6), (1, 7)]~.
                            If we were to search for a value, we can just do a binary search instead of a sequential scan.
                    \item \textbf{B-Trees}
                        \subitem
                            Databases are typically I/O bound (for disk), therefore we want to minimise page I/O operations.
                            There are also many equality lookups, as well as many updates.
                            Databases use high-fanout trees to minimise page I/Os.
                            We want the node of a tree to ideally be exactly the same size as a page (if we access a page, we might as well get the maximum amount of information out).
                            Generally, a node in a B-Tree contains pairs (pivots) of keys and payloads, as well as child pointers to other nodes between the pairs (as well as before and after).
                            \medskip

                            A B-Tree is defined as follows;
                            \begin{itemize}
                                \itemsep0em
                                \item a \textbf{balanced tree} with out-degree $n$ (every node has $n - 1$ keys)
                                \item the \textbf{root} has at least one element
                                \item each \textbf{non-root node} contains at least $\floor{\frac{n - 1}{2}}$ key / value pairs (at least half full)
                            \end{itemize}
                            Unlike other trees, B-Trees grow towards the root.
                            In order to maintain balance, on an insertion the following steps apply;
                            \begin{itemize}
                                \itemsep0em
                                \item find the correct \textbf{leaf-node} to insert by walking the tree and inserting the value
                                \item if this node overflows (the leaf was already full), split the node in two halves
                                    \begin{itemize}
                                        \itemsep0em
                                        \item take one pivot from the middle of the leaf and move it to the parent node
                                        \item the two newly created nodes will become the left and right children of the pivot
                                    \end{itemize}
                                \item if the parent overflows, repeat the procedure on the parent
                                \item if the parent is root, a new root node is introduced (therefore we add more roots, rather than add leaves)
                            \end{itemize}
                            For deletion, the following steps apply;
                            \begin{itemize}
                                \itemsep0em
                                \item find the value to delete
                                    \begin{itemize}
                                        \itemsep0em
                                        \item delete it if it is in a leaf node
                                        \item otherwise, if it is in an internal node, keep a pointer to it, and replace it with the maximum leaf-node value from the left-child (removing the value from the leaf-node) - all modifications are on the leaf level
                                    \end{itemize}
                                \item if the affected leaf node underflows, we need to rebalance the tree bottom-up
                                    \begin{itemize}
                                        \itemsep0em
                                        \item try to obtain an element from a neighbouring node (to the right), move that up one level and make it the new splitting pivot, and take the old splitting pivot from the parent and put it into the leaf node
                                        \item however, if that fails (the neighbour isn't more than half full), the nodes can be merged and the parent splitting key can be removed
                                        \item if that causes an underflow, keep rebalancing upwards
                                    \end{itemize}
                            \end{itemize}
                            While B-Trees can support ranges, it is complicated and requires going up and down the tree.
                            This causes many node traversals, however node sizes are usually the same as page sizes, therefore traversals translate into page faults (which we want to minimise).
                            Leaf pointers also aren't used, and most of the data lives in leaf nodes, therefore there is some space wasted.
                    \item \textbf{B$^+$-Trees}
                        \subitem
                            The idea of this is to make range scans faster by keeping data only in the leaves, and linking leaf nodes to the next.
                            Inner-node split values are replicas of leaf-node values.
                            There is now more of a distinction between inner nodes and leaf nodes.
                            The child pointers of the inner nodes are to other nodes, whereas the child pointers of leaf nodes (to the left) are now the payloads for a given key, and the right-most child pointer points to the next leaf.
                            As such, the leaf nodes now form a sorted sequence.
                            \medskip

                            The balancing is largely the same as the balancing for regular B-Trees, however the deletion of an inner-node's split values imply a replacement with a new value from the leaf node.
                    \item \textbf{Foreign-Key Index}
                        \subitem
                            In SQL, a foreign key is as follows;
                            ~ALTER TABLE Orders ADD FOREIGN KEY (BookID\_index) REFERENCES Book(ID)~.
                            FK constraints specify that for for every value that occurs in an attribute of a table, there is exactly one value in the PK column of another table.
                            This constraint must be done by the DBMS, on an insertion or update, the DBMS needs to look up the PK value, and instead of storing the value, the DBMS could store a pointer to the referenced PK or tuple.
                            We can think of this as equivalent to a pointer, and we can consider these as pre-calculated joins (which is often used since most joins are PK/FK joins from normalisation).
                            \medskip

                            Generally there are very few downsides - they cause insignificant work under updates, they do not cost much more space, and doesn't take more effort for query optimisation.
                \end{itemize}
\end{document}