\documentclass[a4paper, 12pt]{article}
% packages
\usepackage{amssymb}
\usepackage[fleqn]{mathtools}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{bussproofs}
\usepackage{xcolor}
\usepackage[margin=1.3cm]{geometry}
\usepackage{logicproof}
\usepackage{diagbox}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{lstautogobble}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{stmaryrd}
\usetikzlibrary{arrows, shapes.gates.logic.US, circuits.logic.US, calc, automata, positioning}

% shorthand for verbatim
\catcode`~=\active
\def~#1~{\texttt{#1}}

% code listing
\lstdefinestyle{main}{
    numberstyle=\tiny,
    breaklines=true,
    showspaces=false,
    showstringspaces=false,
    tabsize=2,
    numbers=left,
    basicstyle=\ttfamily,
    columns=fixed,
    fontadjust=true,
    basewidth=0.5em,
    autogobble,
    xleftmargin=3.0ex,
    mathescape=true
}
\newcommand{\dollar}{\mbox{\textdollar}} %
\lstset{style=main}

% augmented matrix
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
\hskip -\arraycolsep
\let\@ifnextchar\new@ifnextchar
\array{#1}}
\makeatother

% ceiling / floor
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% custom commands
\newcommand{\indefint}[2]{\int #1 \, \mathrm{d}#2}
\newcommand{\defint}[4]{\int_{#1}^{#2} #3 \, \mathrm{d}#4}
\newcommand{\dif}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\newcommand{\limit}[2]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle{\lim_{#1 \to #2}}$}}}
\newcommand{\summation}[3]{\sum\limits_{#1}^{#2} #3}
% \newcommand{\summation}[3]{\raisebox{0.5ex}{\scalebox{0.5}{$\displaystyle{\sum\limits_{#1}^{#2} #3}$}}}
\newcommand{\intbracket}[3]{\left[#3\right]_{#1}^{#2}}
\newcommand{\ulsmash}[1]{\underline{\smash{#1}}}

\newcommand{\powerset}[0]{\wp}
\renewcommand{\emptyset}[0]{\varnothing}
\newcommand{\la}[0]{\langle}
\newcommand{\ra}[0]{\rangle}

\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\rowt}[1]{\begin{bmatrix}
    #1
\end{bmatrix}^\top}

\newcommand{\unaryproof}[2]{\AxiomC{#1} \UnaryInfC{#2} \DisplayProof}
\newcommand{\binaryproof}[3]{\AxiomC{#1} \AxiomC{#2} \BinaryInfC{#3} \DisplayProof}
\newcommand{\trinaryproof}[4]{\AxiomC{#1} \AxiomC{#2} \AxiomC{#3} \TrinaryInfC{#4} \DisplayProof}

% no indent
\setlength\parindent{0pt}
\setlength\itemsep{0em}

% reasoning proofs
\usepackage{ltablex}
\usepackage{environ}
\keepXColumns
\NewEnviron{reasoning}{
    \begin{tabularx}{\textwidth}{rlX}
        \BODY
    \end{tabularx}
}
\newcommand{\proofline}[3]{$(#1)$ & $#2$ & \hfill #3 \smallskip \\}
\newcommand{\proofarbitrary}[1]{& take arbitrary $#1$ \smallskip \\}
\newcommand{\prooftext}[1]{\multicolumn{3}{l}{#1} \smallskip \\}
\newcommand{\proofmath}[3]{$#1$ & = $#2$ & \hfill #3 \smallskip \\}
\newcommand{\proofiff}[3]{$#1$ & $\Leftrightarrow$ $#2$ & \hfill #3 \smallskip \\}
\newcommand{\prooftherefore}[1]{& $\therefore #1$ \smallskip \\}
\newcommand{\proofbc}[0]{\prooftext{\textbf{Base Case}}}
\newcommand{\proofis}[0]{\prooftext{\textbf{Inductive Step}}}

% reasoning er diagrams
\newcommand{\nattribute}[4]{
    \node[draw, state, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\mattribute}[4]{
    \node[draw, state, accepting, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\dattribute}[4]{
    \node[draw, state, dashed, inner sep=0cm, minimum size=0.2cm, label=#3:{#4}] (#1) at (#2) {};
}
\newcommand{\entity}[3]{
    \node[] (#1-c) at (#2) {#3};
    \node[inner sep=0cm] (#1-l) at ($(#1-c) + (-1, 0)$) {};
    \node[inner sep=0cm] (#1-r) at ($(#1-c) + (1, 0)$) {};
    \node[inner sep=0cm] (#1-u) at ($(#1-c) + (0, 0.5)$) {};
    \node[inner sep=0cm] (#1-d) at ($(#1-c) + (0, -0.5)$) {};
    \draw
    ($(#1-c) + (-1, 0.5)$) -- ($(#1-c) + (1, 0.5)$) -- ($(#1-c) + (1, -0.5)$) -- ($(#1-c) + (-1, -0.5)$) -- cycle;
}
\newcommand{\relationship}[3]{
    \node[] (#1-c) at (#2) {#3};
    \node[inner sep=0cm] (#1-l) at ($(#1-c) + (-1, 0)$) {};
    \node[inner sep=0cm] (#1-r) at ($(#1-c) + (1, 0)$) {};
    \node[inner sep=0cm] (#1-u) at ($(#1-c) + (0, 1)$) {};
    \node[inner sep=0cm] (#1-d) at ($(#1-c) + (0, -1)$) {};
    \draw
    ($(#1-c) + (-1, 0)$) -- ($(#1-c) + (0, 1)$) -- ($(#1-c) + (1, 0)$) -- ($(#1-c) + (0, -1)$) -- cycle;
}

% actual document
\begin{document}
    \section*{CO145 - Mathematical Methods}
        \subsection*{Prelude}
            The content discussed here is part of CO145 - Mathematical Methods (Computing MEng); taught by Michael Huth, and Mario Berta, in Imperial College London during the academic year 2018/19. The notes are written for my personal use, and have no guarantee of being correct (although I hope it is, for my own sake). This should be used in conjunction with the lecture notes. This module differs as there isn't as much new content, but it requires practice - as such, I will likely be including worked examples for my own benefit (which are probably incorrect).
            \medskip

            Also, the formatting of this document will be a mess. I'd waste too much time trying to figure out how to adjust line spacing otherwise.
        \subsection*{Sequences}
            \subsubsection*{Formal Definition of a Limit}
                A sequence $a_n$, for $n \geq 1$, converges to some limit $l \in \mathbb{R}$ if, and only if, we can prove $\forall \epsilon > 0 [\exists N_\epsilon \in \mathbb{N} [\forall n > N_\epsilon [|a_n - l| < \epsilon]]]$.
                \medskip

                To show convergence for the sequence $a_n = \frac{1}{n}$, we need to first make a guess for the limit - suppose $l = 0$. We can now attempt to find some $N_\epsilon$. As $\frac{1}{n} - 0$ is positive for all $n \in \mathbb{N}$, we can drop the absolute, thus it's sufficient to find $n$ such that $\frac{1}{n} < \epsilon$. Since both are positive (hence non-zero), we can take reciprocals on both sides, to get $n > \frac{1}{\epsilon}$. However, we are restricted by the fact that $n$ must be an integer, hence it follows $N_\epsilon = \ceil{\frac{1}{\epsilon}}$. For any value of $\epsilon$, we can get some $N_\epsilon$ with the function, thus it proves that a limit exists.
            \subsubsection*{Common Converging Sequences}
                Note that for all of these, we are implicitly saying $\limit{n}{\infty}$, and that $a_n \to 0$.
                \begin{center}
                    \begin{tabular}{l|l|c}
                        $a_n$ & condition & $N_\epsilon$ \\
                        \hline
                        $\frac{1}{n^c}$ & for some $c \in \mathbb{R}^+$ & $\ceil{\frac{1}{\epsilon^c}}$ \\
                        $\frac{1}{c^n}$ & for some $c \in \mathbb{R}$, such that $|c| > 1$ & $\ceil{\text{log}_c(\frac{1}{\epsilon})}$ \\
                        $c^n$ & for some $c \in \mathbb{R}$, such that $|c| < 1$ & $\ceil{\text{log}_c(\epsilon)}$ \\
                        $\frac{1}{n!}$ & & \\
                        $\frac{1}{\text{ln}(n)}$ & $n > 1$ & $\ceil{e^{\frac{1}{\epsilon}}}$
                    \end{tabular}
                \end{center}
            \subsubsection*{Combining Sequences}
                Suppose that $a_n \to a$, and $b_n \to b$, as $\limit{n}{\infty}$;
                \begin{itemize}
                    \itemsep0em
                    \item $\limit{n}{\infty}\lambda a_n = \lambda a$ given $\lambda \in \mathbb{R}$
                    \item $\limit{n}{\infty}(a_n + b_n) = a + b$
                    \item $\limit{n}{\infty}(a_nb_n) = ab$
                    \item $\limit{n}{\infty}{\frac{a_n}{b_n}} = \frac{a}{b}$ given $b \neq 0$
                \end{itemize}
                \medskip

                For example, the sequence $a_n = \frac{4n^2 + 3n}{7n^2 + 3n - 2}$, it's trivial to find the limit as $n \to \infty$ by inspection as $\frac{4}{7}$. However, if we divide every term by $n^2$, we end up with $a_n = \frac{4 + \frac{3}{n}}{7 + \frac{3}{n} - \frac{2}{n^2}}$, which we can break into $a_n = \frac{b_n}{c_n}$, where $b_n = 4 + \frac{3}{n}$, and $c_n = 7 + \frac{3}{n} - \frac{2}{n^2}$. Using the rules from above, we can further break down the sequences (but I really cannot be bothered to do so), to a point where we get $a = \frac{4 + 0}{7 + 0 - 0} = \frac{4}{7}$.
            \subsubsection*{Sandwich Theorem}
                In the sandwich theorem, where we want to prove that $\limit{n}{\infty}a_n = l$, we need two sequences that form upper, and lower bounds for $a_n$, namely $u_n$, and $l_n$. If such sequences exist, and satisfy $\exists N \in \mathbb{N}[\forall n \geq N [l_n \leq a_n \leq u_n]]$, and both $\limit{n}{\infty}u_n = \limit{n}{\infty}l_n = l$, then we get $\limit{n}{\infty}a_n = l$.
                \medskip

                For example, consider the sequence $a_n = \frac{\text{cos}(n)}{n}$. We know that $-1 \leq \text{cos}(n) \leq 1$, therefore $l_n = -\frac{1}{n} \leq a_n \leq \frac{1}{n} = u_n$. However, as both $u_n \to 0$, and $l_n \to 0$, when $n \to \infty$, it follows that $\limit{n}{\infty}a_n = 0$.
                \medskip

                The sandwich theorem can be proven by finding ${N_\epsilon}_l$, and ${N_\epsilon}_u$ for $l_n$, and $u_n$ respectively. As they both converge to the same limit, we can justify that for some $N_\epsilon = \text{max}({N_\epsilon}_l, {N_\epsilon}_u)$, any $n > N_\epsilon$, we have $|l_n - l| < \epsilon$, and $|u_n - l| < \epsilon$. By removing the modulus signs, we get $-\epsilon < l_n - l < \epsilon$, and $-\epsilon < l_n - l < \epsilon$. By rearranging this, and knowing $l_n < u_n$ by definition, we have $l - \epsilon < l_n < u_n < l + \epsilon$. In order to apply the sandwich theorem, we have to assume $l_n \leq a_n \leq u_n$, hence it follows $l - \epsilon < l_n \leq a_n \leq u_n < l + \epsilon$. This can then be arranged to get $-\epsilon < a_n - l < \epsilon$, thus for all $n > N_\epsilon$, $|a_n - l| < \epsilon$.
            \subsubsection*{Ratio Tests}
                The follow tests allow us to check if a sequence $a_n$ converges to 0, or diverges. If we want to verify a sequence $\limit{n}{\infty}b_n = l$, where $l \neq 0$, we can reformulate it as $a_n = b_n - l$, and prove it for $a_n$.
                \medskip

                If $|\frac{a_{n+1}}{a_n}| \leq c < 1$, for $c \in \mathbb{R}$, where $n$ is sufficiently large, then $\limit{n}{\infty}a_n = 0$. Conversely, if $|\frac{a_{n+1}}{a_n}| \geq c > 1$, then $a_n$ diverges.
                \medskip

                Suppose $a_n = 2^{-n}$, in order to show it converges, we want to find some $c$, such that $|\frac{2^{-(n + 1)}}{2^{-n}}| \leq c < 1$. By arithmetic, we can say $|\frac{2^{-(n + 1)}}{2^{-n}}| = \frac{1}{2} \leq c < 1$. As there exists real $c = \frac{1}{2} < 1$, we can conclude that $a_n$ converges to 0.
                \medskip

                However, we cannot prove that $a_n = \frac{1}{n!}$ converges, despite it being a fairly straightforward proof with just the standard ration tests. For example, we would end up with $|\frac{a_{n + 1}}{a_n}| = \frac{1}{n + 1}$. While this is fairly conclusive that it's less than 1, we don't get a constant $c$ which sits between it, and 1.
                \medskip

                Instead of analysing the behaviour of consecutive terms, we take the limit as $r = \limit{n}{\infty}|\frac{a_{n + 1}}{a_n}|$. If $r < 1$, it converges to 0, but if $r > 1$, it then diverges. Applying this to the previous factorial example, we can clearly see that $\limit{n}{\infty}\frac{1}{n + 1} = 0$, therefore $a_n$ converges to 0.
            \subsubsection*{Manipulating Absolute Values}
                A useful decomposition with absolute limits is $|x| < a \Leftrightarrow -a < x < a$. Further useful properties are;
                \begin{itemize}
                    \itemsep0em
                    \item $|xy| = |x| \cdot |y|$
                    \item $|\frac{x}{y}| = \frac{|x|}{|y|}$
                    \item $|x + y| \leq |x| + |y|$ \hfill triangle inequality
                    \item $|x - y| \geq ||x| - |y||$
                \end{itemize}
            \subsubsection*{Properties of Real Numbers}
                Suppose we have some set $S$ of real numbers, then the following properties hold;
                \begin{itemize}
                    \itemsep0em
                    \item $u$ is an upper bound on $S$ if $\forall s \in S [u \geq s]$
                    \item $l$ is a lower bound on $S$ if $\forall s \in S [l \leq s]$
                    \item $S$ is bounded above if it has an upper bound, bounded below if it has a lower bound, and bounded if it has both
                    \item a set may have no bounds, or many, therefore we say $\text{sup}(S)$ is the least upper bound of $S$, and $\text{inf}(S)$ to denote the greatest upper bound of $S$
                    \item if a set has no upper bound, then $\text{sup}(S) = \infty$, and similarly if there is no lower bound, then $\text{inf}(S) = -\infty$
                \end{itemize}
                The fundamental axiom of analysis states that if an increasing sequence of reals is bounded above, then it must converge.
        \subsection*{Series}
            A series is a sum of a sequence - we consider an infinite series as a summation in the form $S = \summation{n = 1}{\infty}{a_n}$.
            \smallskip

            We can also take a partial sum, where we sum from 1 to $n$ instead of $\infty$ as $S_n = \summation{i = 1}{n}{a_i}$.
            \medskip

            Since we're dealing with finite numbers at this point, we can consider $S_n$ as a sequence - thus whether a series converges, or diverges, depends on whether the sequence of partial sums converge, or diverge. If we know that $\forall i \geq 1 [a_i > 0]$, then the partial sum is an increasing sequence where $S_1 < S_2 < S_3 < ...$, if this is bounded above, then it must converge (from the axiom of analysis).
            \medskip

            For a series to have a chance of converging, we require $\limit{n}{\infty}a_n = 0$. However, just proving that is insufficient for concluding a series converges. It's important to note that we don't really care about the first values of a series, only how it behaves on large numbers, as it tends to infinity.
            \smallskip

            If $S$ converges, or diverges, then $\summation{n = N}{\infty}{a_n}$ also converges, or diverges, as it's equivalent to $S - S_{N - 1}$.
            \smallskip

            \subsubsection*{Geometric Series}
                To determine whether a limit $G$ exists for the geometric series, we need to determine that the partial sums converge. Hence we do the proof on the right hand side. It's important to note that the only term that changes based on $n$ is $x^{n + 1}$, therefore we can conclude that $G$ must converge when $|x| < 1$ (from earlier results).
                \smallskip

                Let us represent the geometric series as $G = \summation{n = 1}{\infty}{x^n}$. Should a limit $G$ exist - see the left side;
                \begin{center}
                    \vspace{-\baselineskip}
                    \begin{minipage}{0.485\textwidth}
                        \begin{align*}
                            G & = \summation{n = 1}{\infty}{x^n} \\
                            & = x + \summation{n = 2}{\infty}{x^n} \\
                            & = x + x\summation{n = 1}{\infty}{x^n} \\
                            & = x + xG \\
                            & = \frac{x}{1 - x}
                        \end{align*}
                    \end{minipage} \
                    \begin{minipage}{0.485\textwidth}
                        \begin{align*}
                            G_n & = \summation{i = 1}{n}{x^i} \\
                            & = x + \summation{i = 2}{n}{x^i} \\
                            & = x + x\summation{i = 1}{n - 1}{x^i} \\
                            & = x + x(G_n - x^n) \\
                            & = \frac{x - x^{n + 1}}{1 - x}
                        \end{align*}
                    \end{minipage}
                \end{center}
            \subsubsection*{Harmonic Series}
                The harmonic series is often used in comparison tests to prove that another series diverges.
                \smallskip

                Let harmonic series be written as $S = \summation{n = 1}{\infty}{\frac{1}{n}} = 1 + \frac{1}{2} + \underbrace{(\textstyle\frac{1}{3} + \frac{1}{4})}_{> \frac{1}{4} + \frac{1}{4}} + \underbrace{(\textstyle\frac{1}{5} + \frac{1}{6} + \frac{1}{7} + \frac{1}{8})}_{> \frac{1}{8} + \frac{1}{8} + \frac{1}{8} + \frac{1}{8}} + ... > 1 + \frac{1}{2} + \frac{1}{2} + \frac{1}{2} + ...$
                \smallskip

                Therefore, with partial sums, we get $S_n = \summation{i = 1}{2^n}{\frac{1}{i}} > 1 + \frac{n}{2}$. As $S_n$ clearly diverges, $S$ must also diverge.
            \subsubsection*{Inverse Squares}
                Suppose we have the sum of squares $S = \summation{n = 1}{\infty}{\frac{1}{n^2}}$, and an auxiliary partial sum $T_n = \summation{i = 1}{n}{\frac{1}{i(i + 1)}}$.
                \smallskip

                To prove that $S$ converges, we should consider $T_n = 1 - \frac{1}{n + 1}$ (write out the first few terms and a pattern should be spotted). Hence $\limit{n}{\infty}T_n = 1$. Using this, we can argue how the partial sums of the series of inverse squares are bounded. It's trivial to say the following inequality; $\frac{1}{i(i + 1)} < \frac{1}{i^2} < \frac{1}{i(i - 1)}$.
                \smallskip

                If we sum this inequality from $i = 2$ to $n$, we can then justify the following - it's important to note that the sums on the left, and right, differ by only the first, and last term (therefore we can reduce it to a partial sum);
                \medskip

                $\phantom{\Leftrightarrow} \frac{1}{2(3)} + \frac{1}{3(4)} + \frac{1}{4(5)} + ... + \frac{1}{n(n + 1)} < \summation{i = 2}{n}{\frac{1}{i^2}} < \frac{1}{1(2)} + \frac{1}{2(3)} + \frac{1}{3(4)} + ... + \frac{1}{n(n - 1)}$
                \smallskip

                $\Leftrightarrow T_n - \frac{1}{2} < \summation{i = 2}{n}{\frac{1}{i^2}} < T_{n - 1}$
                \smallskip

                $\Leftrightarrow T_n + \frac{1}{2} < \summation{i = 1}{n}{\frac{1}{i^2}} < T_{n - 1} + 1$
                \medskip

                $\Leftrightarrow \frac{3}{2} + \frac{1}{n + 1} < S_n < 2 + \frac{1}{n}$
                \medskip

                Therefore, it follows that the partial sums are bounded by $\frac{3}{2}$, and 2. The existence of an upper bound suggests the sequence converges, therefore the series must also converge.
            \subsubsection*{Important Series}
                These examples will be useful for the comparison tests covered later on. We can use these results to aid us in proving whether some other series converges, or diverges.
                \begin{center}
                    \begin{tabular}{l|l|l|l}
                        name & $S$ & condition & diverges or converges \\
                        \hline
                        harmonic series & $\summation{n = 1}{\infty}{\frac{1}{n}}$ & & diverges \\
                        harmonic primes & $\summation{p : \text{prime}}{}{\frac{1}{p}}$ & & diverges \\
                        geometric series & $\summation{n = 1}{\infty}{x^n}$ & $|x| \geq 1$ & diverges \\
                        geometric series & $\summation{n = 1}{\infty}{x^n}$ & $|x| < 1$ & converges \\
                        inverse squares series & $\summation{n = 1}{\infty}{n^2}$ & & converges \\
                        ??? & $\summation{n = 1}{\infty}{\frac{1}{n^c}}$ & $c > 1$ & converges
                    \end{tabular}
                \end{center}
            \subsubsection*{Convergence Tests}
                For brevity, suppose we have the following;
                \begin{itemize}
                    \itemsep0em
                    \item $S = \summation{i = 1}{\infty}{a_i}$ \hfill a series we want to reason about, and $a_i \geq 0$
                    \item $\summation{i = 1}{\infty}{c_i}$ \hfill a series we have already established converges to $c$
                    \item $\summation{i = 1}{\infty}{d_i}$ \hfill a series we have already established diverges
                \end{itemize}
                In the \textbf{comparison test}, we have some $\lambda > 0$, and a $N \in \mathbb{N}$. If $\forall i > N [a_i \leq \lambda c_i]$, then $S$ converges. On the other hand, if $\forall i > N [a_i \geq \lambda d_i]$, then $S$ diverges.
                \medskip

                To avoid having to find such a $\lambda$, we can use the \textbf{limit comparison test}, in which we can say if $\limit{i}{\infty}\frac{a_i}{c_i}$ exists, then $S$ converges, but if $\limit{i}{\infty}\frac{d_i}{a_i}$ exists, then $S$ diverges.
                \medskip

                Another useful test for finding convergence, without needing a pre-established series, is \textbf{D'Alembert's ratio test}. Once again, we consider this from some point $N \in \mathbb{N}$, where the cases are;
                \begin{enumerate}[1.]
                    \itemsep0em
                    \item $\forall i \geq N [\frac{a_{i + 1}}{a_i} \geq 1]$, then $S$ diverges
                    \item there exists a $k$ such that $\forall i \geq N [\frac{a_{i + 1}}{a_i} \leq k < 1]$, then $S$ converges
                \end{enumerate}
                This is similar to the comparison test for sequences, and once again the requirement of finding some $k$ between the ratio, and 1 can be avoided by taking limits. This leads to \textbf{D'Alembert's limit ratio test}, in which we have the following cases;
                \begin{enumerate}[1.]
                    \itemsep0em
                    \item $\limit{i}{\infty}\frac{a_{i + 1}}{a_i} > 1$, then $S$ diverges
                    \item $\limit{i}{\infty}\frac{a_{i + 1}}{a_i} = 1$, then it is inconclusive
                    \item $\limit{i}{\infty}\frac{a_{i + 1}}{a_i} < 1$, then $S$ converges
                \end{enumerate}
                The second case, in which it is inconclusive, can be illustrated by setting $a_n = 1$, which obviously causes $S$ to diverge, or by setting $a_n = \frac{1}{n^2}$ which causes $S$ to converge. However, both lead to a limit test of 1.
                \medskip

                The \textbf{integral test} for a sequence, where $a_n = f(n)$ is a decreasing function relies on the following idea;
                \begin{center}
                    $\summation{n = 1}{\infty}{a_{n + 1}} < \defint{1}{\infty}{f(x)}{x} < \summation{n = 1}{\infty}{a_n}$
                \end{center}
                In practice, if the integral were to diverge once evaluated, the series would also diverge by the right hand inequality, but if it were to converge, then the series would converge with the left hand inequality.
            \subsubsection*{Absolute Convergence}
                Consider a series with negative $a_n$ values, and another series $S^\prime = \summation{n = 1}{\infty}{|a_n|}$. If $S^\prime$ converges, so does $S$.
                \smallskip

                However, the same isn't true the other way around, for example, when we have $a_n = (-1)^{n - 1} \frac{1}{n}$, $S$ converges to $\text{ln}(2)$, however, doing this with the absolute series, $S^\prime$, it would diverge, as if it were the normal harmonic series. In order to test for \textbf{absolute} convergence - we have the following cases;
                \begin{enumerate}[1.]
                    \itemsep0em
                    \item if $\limit{n}{\infty}|\frac{a_{n + 1}}{a_n}| > 1$ then $\summation{n = 1}{\infty}{a_n}$ diverges
                    \item if $\limit{n}{\infty}|\frac{a_{n + 1}}{a_n}| = 1$ then $\summation{n = 1}{\infty}{a_n}$ can converge, or diverge
                    \item if $\limit{n}{\infty}|\frac{a_{n + 1}}{a_n}| < 1$ then $\summation{n = 1}{\infty}{a_n}$ converges absolutely, and therefore also converges
                \end{enumerate}
            \subsubsection*{Power Series, and Radius of Convergence}
                We can represent a function as a power series; $f(x) = \summation{n = 0}{\infty}{a_nx^n}$.
                \smallskip

                Consider the case where $f(x) = \frac{x}{1 - x}$, which has the expansion $\summation{n = 1}{\infty}{x^n}$, which only converges for $|x| < 1$.
                \smallskip

                Take the power series $S = \summation{n = 1}{\infty}{n^2 x^n}$, and apply the \textbf{D'Alembert ratio test}, we'd get the following;
                \begin{align*}
                    \limit{n}{\infty} |\frac{a_{n + 1}}{a_n}| & = \limit{n}{\infty}|\frac{(n + 1)^2 x^{n + 1}}{n^2 x^n}| \\
                    & = \limit{n}{\infty} |x(1 + \frac{1}{n})^2| \\
                    & = |x|
                \end{align*}
                As we want to establish convergence, we set the result $|x| < 1$, therefore it follows that the ratio of convergence for this particular power series is 1.
        \subsection*{Power Series}
            As previously mentioned, a function can be represented in a series expansion on $x$; $f(x) = \summation{n = 0}{\infty}{a_nx^n}$.
            \subsubsection*{Maclaurin Series}
                Let some function $f(x) = a_0 + a_1x + a_2x^2 + a_3x^3 + ...$, then we can easily work out $a_0$, by setting $x = 0$, such that $f(0) = a_0$. By differentiating the function, we get $f^\prime(x) = a_1 + 2a_2x + 3a_3x^2 + ...$, therefore it follows that $f^\prime(0) = a_1$. By differentiating again,we get $f^{\prime\prime}(x) = 2a_2 + 2 \cdot 3a_3x + ...$, therefore it follows that $\frac{f^{\prime\prime}(0)}{2!} = a_2$. Hence, generally $a_n = \frac{f^{(n)}(0)}{n!}$, where $n \geq 0$.
                \medskip

                A \textbf{Maclaurin series} is the representation of $f(x) = \summation{i = 0}{\infty}{a_ix^i}$.
                \smallskip

                This function can then be differentiated $n$ times to get $f^{(n)}(x) = \summation{i = n}{\infty}{a_i(i - 1)...(i - n + 1)x^{i - n}}$.
                \smallskip

                From this, we can see that $f^{(n)}(0) = n! a_n$, hence $f(x) = \summation{n = 0}{\infty}{f^{n}(0)\frac{x^n}{n!}}$.
                \smallskip

                For example, if we take $f(x) = \text{ln}(1 + x)$, and differentiate repeatedly;
                \begin{reasoning} % too lazy to make a new environment
                    \proofmath{f(x)}{\text{ln}(1 + x)}{$f(0) = 0$}
                    \proofmath{f^\prime(x)}{(1 + x)^{-1}}{$f^\prime(0) = 1$}
                    \proofmath{f^{\prime\prime}(x)}{(-1)(1 + x)^{-2}}{$f^{\prime\prime}(0) = -1$}
                    \proofmath{f^{\prime\prime\prime}(x)}{(-2)(-1)(1 + x)^{-3}}{$f^{\prime\prime}(0) = 2$}
                    \proofmath{f^{(n)}(x)}{\frac{(-1)^{n - 1}(n - 1)!}{(1 + x)^n}}{$f^{(n)}(0) = (-1)^{n - 1}(n - 1)!$}
                    $a_n$ & = $\frac{(-1)^{n - 1}}{n}$
                \end{reasoning}
                Therefore, we can justify that $f(x) = \text{ln}(1 + x) = \frac{x}{1} - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + ...$. By setting $x = -1$, we'd get $-\infty$, and the power series would become the harmonic series, which we know to diverge. However, by setting $x = 1$, we get $\text{ln}(2)$, is the convergence of the alternating harmonic series (as previously mentioned, in the ratio test example).
            \subsubsection*{Taylor Series}
                When we're interested in values around a certain point, we can use a more generalised version of a \textbf{Maclaurin series}. The general result, where we apply the series around $a$ is;
                \smallskip

                $f(x) = f(a) + \frac{f^{(1)}(a)}{1!}(x - a)^1 + \frac{f^{(2)}(a)}{2!}(x - a)^2 + ... = \summation{n = 0}{\infty}{\frac{f^{(n)}(a)}{n!}(x - a)^n}$
                \smallskip

                For example, if we take $f(x) = \text{ln}(x)$ around $a$, and differentiate repeatedly;
                \begin{reasoning} % too lazy to make a new environment
                    \proofmath{f(x)}{\text{ln}(x)}{$f(2) = \text{ln}(2)$}
                    \proofmath{f^\prime(x)}{\frac{1}{x}}{$f^\prime(2) = \frac{1}{2}$}
                    \proofmath{f^{\prime\prime}(x)}{(-1)\frac{1}{x^2}}{$f^{\prime\prime}(2) = -\frac{1}{4}$}
                    \proofmath{f^{\prime\prime\prime}(x)}{(-2)(-1)\frac{1}{x^3}}{$f^{\prime\prime}(2) = \frac{1}{4}$}
                    \proofmath{f^{(n)}(x)}{\frac{(-1)^{n - 1}(n - 1)!}{x^n}}{$f^{(n)}(2) = \frac{(-1)^{n - 1}(n - 1)!}{2^n}$}
                    $a_n$ & = $\frac{(-1)^{n - 1}}{n2^n}$
                \end{reasoning}

                For example, we can approximate $\text{ln}(x)$ around 2 as; $\text{ln}(x) = \text{ln}(2) + \summation{n = 1}{\infty}{\frac{(-1)^{n - 1}}{n2^n}(x - 2)^2}$.
                \smallskip

                It's important to note the bounds for this, as we start from $n = 1$, and manually state the constant term, since that doesn't fall within the "pattern" we had. We use the same technique to calculate the radius of convergence, as follows;
                \begin{reasoning}
                    \proofmath{\limit{n}{\infty}|\frac{a_{n + 1}}{a_n}|}{\limit{n}{\infty}|\frac{\frac{(x - 2)^{n + 1}}{(n + 1)2^{n + 1}}}{\frac{(x - 2)^n}{n2^n}}|}{}
                    \proofmath{}{\limit{n}{\infty}\frac{n}{n + 1} \cdot \frac{|x - 2|}{2}}{}
                    \proofmath{}{\frac{|x - 2|}{2}}{}
                \end{reasoning}
                And then, by setting $\frac{|x - 2|}{2} < 1$,we have a radius of convergence of 2, around $x = 2$. Generally, the radius is limited by the closest singularity (where a point is not defined, or isn't well-behaved).
            \subsubsection*{Taylor Series Error Term}
                In order to ensure termination, instead of summing to $\infty$, we often truncate at some arbitrary $k^\text{th}$ term.
                \begin{center}
                    $f(x) = \summation{n = 0}{\infty}{\frac{f^{(n)}(a)}{n!}(x - a)^n} = \summation{n = 0}{k}{\frac{f^{(n)}(a)}{n!}(x - a)^n} + \underbrace{\textstyle\frac{f^{k + 1}(c)}{(k + 1)!}(x - a)^{k + 1}}_\text{Lagrange error term}$
                \end{center}
                The Lagrange error term replaces the tail of the infinite series, from, which would otherwise be a sum from $k + 1$ to $\infty$. The constant $c$ lies between $x$, and $a$. By taking the bound $a < c < x$, or $x < c < a$, it can be used to generate the worst-case error.
                \medskip

                The mean value theorem states that there is some point $c$ between $x$, and $a$, such that $f^\prime(c) = \frac{f(x) - f(a)}{x - a}$, therefore we can get $f(x) = f(a) + (x - a)f^\prime(c)$.
            \subsubsection*{Cauchy Error Term}
                Define the set of partial sums of the Taylor series as $F_k(t) = \summation{n = 0}{k}{\frac{f^{(n)}(t)}{n!}(x - t)^n}$, where $t$ is the offset.
                \smallskip

                It follows that $F_k(x) = f(x)$ for all $k$ (justified as every term becomes 0, other than the first term, $n = 0$, where it's $\frac{f(x)}{0!}$). Therefore the error term is derived as follows, where $R_k(x)$ is the remainder after $k$ terms;
                \begin{center}
                    $F_k(x) - F_k(a) = f(x) - \summation{n = 0}{k}{\frac{f^{(n)}(a)}{n!}(x - a)^n} = R_k(x)$
                \end{center}
            \subsubsection*{Power Series Solutions of ODE}
                When we have a differential equation in the form $\dif{y}{x} = ky$, for a constant k, we can consider the power series for $y$. By differentiating term by term, we can compare coefficients;
                \begin{center}
                    $\dif{y}{x} \equiv \dif{}{x}\summation{i = 0}{\infty}{a_ix^i} \equiv \summation{i = 1}{\infty}{a_iix^{i - 1}} \equiv ky \equiv k\summation{i = 0}{\infty}{a_ix^i} \equiv k\summation{i = 1}{\infty}{a_{i - 1}x^{i - 1}} \equiv \summation{i = 1}{\infty}{ka_{i - 1}x^{i - 1}}$
                \end{center}
                By matching coefficients of $x^{i - 1}$, where $i \geq 1$, we have $ka_{i - 1} = a_ii$, therefore we end up with the relation $a_i = \frac{k}{i}a_{i - 1}$. By "expanding" this, we end up with $a_i = \frac{k}{i}a_{i - 1} = \frac{k}{i} \cdot \frac{k}{i - 1}a_{i - 2} = ... = \frac{k^i}{i!}a_0$.
                \medskip

                If we were to have some boundary condition, such as when $x = 0$, $y = 1$, we know that $y(0) = a_0$, hence $a_0 = 1$. Therefore, this can be solved to the following;
                \begin{center}
                    $y = \summation{i = 0}{\infty}{\frac{(kx)^i}{i!}} = e^{kx}$
                \end{center}
        \subsection*{Linear Algebra}
            \subsubsection*{Linear Equation Systems}
                For brevity, instead of writing;
                \smallskip

                $\begin{matrix}
                    ax_1 & + & bx_2 & + & cx_3 & = & j \\
                    dx_1 & + & ex_2 & + & fx_3 & = & k \\
                    gx_1 & + & hx_2 & + & ix_3 & = & l
                \end{matrix}$ \hfill $\begin{matrix}
                    (1) \\ (2) \\ (3)
                \end{matrix}$

                We will write $\mat{A}\vec{x} = \vec{y}$, where $\mat{A} = \begin{bmatrix}
                    a & b & c \\
                    d & e & f \\
                    g & h & i
                \end{bmatrix}$, $\vec{x} = \begin{bmatrix}
                    x_1 \\ x_2 \\ x_3
                \end{bmatrix}$, and $\vec{y} = \begin{bmatrix}
                    j \\ k \\ l
                \end{bmatrix}$
                \medskip

                In general, for a real linear equation system, we have the following possible outcomes;
                \begin{itemize}
                    \itemsep0em
                    \item no solution
                        \subitem $\mat{A} = \begin{bmatrix}
                            1 & 1 & 1 \\
                            1 & -1 & 2 \\
                            2 & 0 & 3
                        \end{bmatrix}$, and $\vec{y} = \begin{bmatrix}
                            3 \\ 2 \\ 1
                        \end{bmatrix}$
                        \subitem By adding equations (1), and (2), we get $2x_1 + 3x_3 = 5$, which contradicts equation (3), hence it is \textbf{inconsistent}, and therefore has no solutions.
                    \item unique solution
                        \subitem $\mat{A} = \begin{bmatrix}
                            1 & 1 & 1 \\
                            1 & -1 & 2 \\
                            0 & 1 & 1
                        \end{bmatrix}$, and $\vec{y} = \begin{bmatrix}
                            3 \\ 2 \\ 2
                        \end{bmatrix}$
                        \subitem By doing equation (1) - equation (3), we get $x_1 = 1$. Adding equations (1), and (2), we end up with $2x_1 + 3_x3 = 5$, however, we know the value of $x_1$, therefore it follows that $x_3 = 1$. Finally, by using this value in equation (3), we have $x_2 = 1$, therefore the only solution is;
                        \subitem $\vec{x} = \begin{bmatrix}
                            1 \\ 1 \\ 1
                        \end{bmatrix}$
                    \item infinite solutions
                        \subitem $\mat{A} = \begin{bmatrix}
                            1 & 1 & 1 \\
                            1 & -1 & 2 \\
                            2 & 0 & 3
                        \end{bmatrix}$, and $\vec{y} = \begin{bmatrix}
                            3 \\ 2 \\ 5
                        \end{bmatrix}$
                        \subitem Note that this is the same $\textbf{A}$ from the first example. However, this time adding equations (1), and (2), is consistent with equation (3). If we let $x_3 = \lambda$, such that $x_3$ is a free variable, with $\lambda \in \mathbb{R}$, we can get both $x_1$, and $x_2$ in terms of $\lambda$. By doing equations (1) - (2), we get $2x_2 - x_3 = 1$, hence $x_2 = \frac{1}{2} + \frac{1}{2}\lambda$. By doing equations (1) + (2), we get $2x_1 + 3x_3 = 5$, which leads to $x_1 = \frac{5}{2} - \frac{3}{2}\lambda$. As such, we have an infinite set of solutions, given $\lambda \in \mathbb{R}$;
                        \subitem $\vec{x} = \begin{bmatrix}
                            \frac{5}{3} \\ \frac{1}{2} \\ 0
                        \end{bmatrix} + \lambda \begin{bmatrix}
                            -\frac{3}{2} \\ \frac{1}{2} \\ 1
                        \end{bmatrix}$
                \end{itemize}
            \subsubsection*{Groups}
                With a set $G$, and some operation $\otimes : G \mapsto G$, the criteria for $(G, \otimes)$ being a \textbf{group} are (1 - 4), and (1 - 5) for an \textbf{Abelian group};
                \begin{enumerate}[1.]
                    \itemsep0em
                    \item \textbf{closure} \hfill $\forall x, y \in G [x \otimes y \in G]$
                    \item \textbf{associativity} \hfill $\forall x, y, z \in G [(x \otimes y) \otimes z = x \otimes (y \otimes z)]$
                    \item \textbf{neutral element} \hfill $\exists e \in G [\forall x \in G [x \otimes e = x \land e \otimes x = x]]$
                    \item \textbf{inverse element} \hfill $\forall x \in G [\exists y \in G [x \otimes y = e \land y \otimes x = e]]$
                        \subitem often written as $y = x^{-1}$, note that this does \textbf{not} mean $y = \frac{1}{x}$, defined with respect to $\otimes$
                    \item \textbf{commutative} \hfill $\forall x, y \in G [x \otimes y = y \otimes x]$
                \end{enumerate}
                For example, we can say $(\mathbb{Z}, +)$ is a group, but $(\mathbb{N}, +)$ is not, since the latter doesn't have an inverse element; for example $-5 \notin \mathbb{N}$, so 5 doesn't have an inverse. While $(\mathbb{R}\backslash\{0\}, \cdot)$ is an Abelian group, $(\mathbb{R}, \cdot)$ isn't a group as 0 doesn't have an inverse element ($\neg \exists x \in \mathbb{R} [0 \cdot x = 1]$). $(\mathbb{R}^n, +)$ is an Abelian groups under addition, as long as addition is defined component-wise.
            \subsubsection*{Matrices}
                We can defined the set of all real-valued $m \times n$ matrices ($m, n \in \mathbb{N}$), where $m$ is the number of rows, and $n$ is the number of columns, as $\mathbb{R}^{m \times n}$. We can refer to each element as $a_{i, j}$, where $i \in [1, m]$, and $j \in [1, n]$, and $a_{i, j} \in \mathbb{R}$; thus giving us the following rectangular matrix;
                \begin{center}
                    $\mat{A} = \begin{bmatrix}
                        a_{1, 1} & a_{1, 2} & \cdots & a_{1, n} \\
                        a_{2, 1} & a_{2, 2} & \cdots & a_{2, n} \\
                        \vdots & \vdots & \ddots & \vdots \\
                        a_{m, 1} & a_{m, 2} & \cdots & a_{m, n}
                    \end{bmatrix}$
                \end{center}
                A $\mathbb{R}^{1 \times n}$ matrix is referred to as a \textbf{row (vector)}, and a $\mathbb{R}^{m \times 1}$ matrix is referred to as a \textbf{column (vector)}, with component-wise addition, we are able to say $(\mathbb{R}^{m \times n}, +)$ is a group.
                \medskip

                Multiplication on matrices, $\mat{A} \in \mathbb{R}^{m \times n}, \mat{B} \in \mathbb{R}^{n \times k}, \mat{C} \in \mathbb{R}^{m \times k}$, where $\mat{C} = \mat{AB}$, is defined as;
                \begin{center}
                    $c_{i, j} = \summation{l = 1}{n}{a_{i, l}b_{l, j}}$, with $i \in [1, m]$, and $j \in [1, k]$
                \end{center}
                In general, matrix multiplication is not commutative, hence $\mat{AB} \neq \mat{BA}$. However, it is associative, and distributive (given that the dimensions match).
                \medskip

                The identity matrix, written as $\mat{I}_n \in \mathbb{R}^{n \times n}$, is all 0s, other than $a_{i, i} = 1$, where $i \in [1, n]$. The identity matrix has the following property; $\mat{I}_m\mat{A} = \mat{A} = \mat{AI}_n$, where $\mat{A} \in \mathbb{R}^{m \times n}$.
                \medskip

                If a square matrix ($\mat{A} \in \mathbb{R}^{n \times n}$) is non-singular, we can find an inverse for it, $\mat{A}^{-1} \in \mathbb{R}^{n \times n}$, such that $\mat{AA}^{-1} = \mat{A}^{-1}\mat{A} = \mat{I}_n$. However, not every matrix has an inverse. We also have the result $(\mat{AB})^{-1} = \mat{B}^{-1}\mat{A}^{-1}$.
                \medskip

                We can define the transpose of $\mat{A} \in \mathbb{R}^{n \times m}$ as $\mat{B} = \mat{A}^\top \in \mat{R}^{m \times n}$, where $b_{j, i} = a_{i, j}$. On the rectangular matrix representation, we are essentially mirroring it on the leading diagonal. A \textbf{square} matrix is symmetric $\Leftrightarrow \mat{A} = \mat{A}^\top$. The general results are $(\mat{A}^\top)^\top = \mat{A}$, $(\mat{A} + \mat{B})^\top = \mat{A}^\top + \mat{B}^\top$, and $(\mat{AB})^\top = \mat{B}^\top\mat{A}^\top$. Also, if $\mat{A}$ is non-singular, then $(\mat{A}^{-1})^\top = (\mat{A}^\top)^{-1}$.
                \medskip

                A multiplication by scalar $\lambda \in \mathbb{R}$, means that for $\mat{A} \in \mathbb{R}^{m \times n}$, and $\mat{B} = \lambda \mat{A}$, it follows that $b_{i, j} = \lambda a_{i, j}$. This operation is distributive, and also associative.
            \subsubsection*{Solving Linear Equation Systems}
                Given the general form of an equation system as;
                \begin{center}
                    $\begin{matrix}
                        a_{1, 1}x_1 & + & a_{1, 2}x_1 & + & \cdots & + & a_{1, n}x_n & = & b_1 \\
                        a_{2, 1}x_1 & + & a_{2, 2}x_2 & + & \cdots & + & a_{2, n}x_n & = & b_2 \\
                        \vdots & & \vdots & & & & \vdots & & \vdots \\
                        a_{m, 1}x_1 & + & a_{m, 2}x_2 & + & \cdots & + & a_{m, n}x_n & = & b_m
                    \end{matrix}$
                \end{center}
                We can easily represent it as $\mat{A}\vec{x} = \vec{b}$, where $\mat{A} \in \mathbb{R}^{m \times n}$, $\vec{x} \in \mathbb{R}^{n (\times 1)}$, and $\vec{b} \in \mathbb{R}^{m (\times 1)}$.
                \medskip

                For example, consider a simple linear equation system as follows;
                \begin{center}
                    $\mat{A}\vec{x} = \begin{bmatrix}
                        1 & 0 & 8 & -4 \\
                        0 & 1 & 2 & 12
                    \end{bmatrix} \begin{bmatrix}
                        x_1 \\ x_2 \\ x_3 \\ x_4
                    \end{bmatrix} = \begin{bmatrix}
                        42 \\ 8
                    \end{bmatrix}$
                \end{center}
                Let us also say $\mat{A} = \begin{bmatrix}[c|c|c|c]
                    \vec{c_1} & \vec{c_2} & \vec{c_3} & \vec{c_4}
                \end{bmatrix}$, where $\vec{c_i}$ represents the $i^\text{th}$ column of $\mat{A}$, hence $\vec{b} = \summation{i = 1}{4}{x_i \vec{c_i}}$.
                \smallskip

                In order to find a particular solution, we can easily create $\vec{b}$ as a linear combination of $\vec{c_1}$, and $\vec{c_2}$, such that $\vec{b} = 42\vec{c_1} + 8\vec{c_2} + 0\vec{c_3} + 0\vec{c_4}$, therefore we can get a \textbf{particular} solution; $\vec{x} = \rowt{42 & 8 & 0 & 0}$.


                This doesn't capture all the solutions of this equation system - if there are more unknowns than equations, then there are an infinite number of solutions. Recall that $\vec{b} + \vec{0} = \vec{b}$, hence we can attempt to find non-trivial $\vec{0}$s to generate the set of all solutions.
                \medskip

                Using the fact that $\mat{A}$ is already in \textbf{row echelon form}, we can try say that $\vec{c_3} = 8\vec{c_1} + 2\vec{c_2}$, therefore, it follows that $\vec{0} = 8\vec{c_3} + 2\vec{c_2} - \vec{c_3} + 0\vec{c_4}$, as such we've managed to create a zero solution when $\vec{x} = \rowt{8 & 2 & -1 & 0}$. We can do the same with $\vec{c_4}$, such that $\vec{c_4} = -4\vec{c_1} + 12\vec{c_2} \Leftrightarrow \vec{0} = -4\vec{c_1} + 12\vec{c_2} + 0\vec{c_3} - \vec{c_4}$, hence $\vec{x} = \rowt{-4 & 12 & 0 & -1}$ is also a zero solution. With these two $\vec{0}$s, we can say that the general solution is as follows (where $\lambda_1, \lambda_2 \in \mathbb{R}$);
                \begin{center}
                    $\vec{x} = \begin{bmatrix}
                        42 \\ 8 \\ 0 \\ 0
                    \end{bmatrix} + \lambda_1 \begin{bmatrix}
                        8 \\ 2 \\ -1 \\ 0
                    \end{bmatrix} + \lambda_2 \begin{bmatrix}
                        -4 \\ 12 \\ 0 \\ -1
                    \end{bmatrix}$
                \end{center}
                In this solution, we first find a \textbf{particular} solution for $\mat{A}\vec{x} = \vec{b}$, and then combine this with \textbf{all} solutions to $\mat{A}\vec{x} = \vec{0}$. This was fairly trivial to solve, as the matrix was already in a special form. Generally, equation systems won't be given to us like this, however we can transform it into this form by \textbf{Gaussian elimination}.
            \subsubsection*{Elementary Transformations}
                We are allowed to apply the following operations, which let us transform the equation system into a simpler state, but preserve the solution set;
                \begin{itemize}
                    \itemsep0em
                    \item swap two equations (which means we can swap rows in the matrix)
                    \item multiply an equation (matrix row) by a non-zero constant $\lambda \in \mathbb{R}\backslash\{0\}$
                    \item add (or subtract) an equation (row) to / from another equation (row)
                \end{itemize}
                We can say that a matrix is in \textbf{row echelon form (REF)}, if all zero rows are below the non-zero rows, and the pivot of each row is \textbf{strictly} to the right of the pivot in the row above. The pivot of a row is defined as the first non-zero element in a given row, reading left-to-right.
                \medskip

                Consider the following system of equations, and the result of it after Gaussian elimination;
                \begin{center}
                    \setcounter{MaxMatrixCols}{20}
                    $\begin{matrix}
                        -2x_1 & + & 4x_2 & - & 2x_3 & - & x_4 & + & 4x_5 & = & -3 \\
                        4x_1 & - & 8x_2 & + & 3x_3 & - & 3x_4 & + & x_5 & = & 2 \\
                        x_1 & - & 2x_2 & + & x_3 & - & x_4 & + & x_5 & = & 0 \\
                        x_1 & - & 2x_2 & + & 0x_3 & - & 3x_4 & + & 4x_5 & = & a \\
                        & & & & & \leadsto & & & & & \\
                        x_1 & - & 2x_2 & + & x_3 & - & x_4 & + & x_5 & = & 0 \\
                        & & & & x_3 & - & x_4 & + & 3x_5 & = & -2 \\
                        & & & & & & x_4 & - & 2x_5 & = & 1 \\
                        & & & & & & & & 0 & = & a + 1
                    \end{matrix}$
                \end{center}
                We can now work on the augmented matrix to transform it into \textbf{REF};
                % I forgot how painful doing matrices on LaTeX was.
                \begin{tabularx}{\textwidth}{llll}
                    $\begin{bmatrix}[ccccc|c]
                        -2 & 4 & -2 & -1 & 4 & -3 \\
                        4 & -8 & 3 & -3 & 1 & 2 \\
                        1 & -2 & 1 & -1 & 1 & 0 \\
                        1 & -2 & 0 & -3 & 4 & a
                    \end{bmatrix}\begin{matrix}
                        \leftrightarrow R_3 \\ {} \\ \leftrightarrow R_1 \\ {}
                    \end{matrix}$ & $\leadsto$ & $\begin{bmatrix}[ccccc|c]
                        1 & -2 & 1 & -1 & 1 & 0 \\
                        4 & -8 & 3 & -3 & 1 & 2 \\
                        -2 & 4 & -2 & -1 & 4 & -3 \\
                        1 & -2 & 0 & -3 & 4 & a
                    \end{bmatrix}\begin{matrix}
                        {} \\ -4R_1 \\ +2R_1 \\ -R_1
                    \end{matrix}$ & $\leadsto$ \medskip \\

                    $\begin{bmatrix}[ccccc|c]
                        1 & -2 & 1 & -1 & 1 & 0 \\
                        0 & 0 & -1 & 1 & -3 & 2 \\
                        0 & 0 & 0 & -3 & 6 & -3 \\
                        0 & 0 & -1 & -2 & 3 & a
                    \end{bmatrix}\begin{matrix}
                        {} \\ {} \\ {} \\ -R_2
                    \end{matrix}$ & $\leadsto$ & $\begin{bmatrix}[ccccc|c]
                        1 & -2 & 1 & -1 & 1 & 0 \\
                        0 & 0 & -1 & 1 & -3 & 2 \\
                        0 & 0 & 0 & -3 & 6 & -3 \\
                        0 & 0 & 0 & -3 & 6 & a - 2
                    \end{bmatrix}\begin{matrix}
                        {} \\ {} \\ {} \\ -R_3
                    \end{matrix}$ & $\leadsto$ \medskip \\

                    $\begin{bmatrix}[ccccc|c]
                        1 & -2 & 1 & -1 & 1 & 0 \\
                        0 & 0 & -1 & 1 & -3 & 2 \\
                        0 & 0 & 0 & -3 & 6 & -3 \\
                        0 & 0 & 0 & 0 & 0 & a + 1
                    \end{bmatrix}\begin{matrix}
                        {} \\ \cdot -1 \\ \cdot -\frac{1}{3} \\ {}
                    \end{matrix}$ & $\leadsto$ & $\begin{bmatrix}[ccccc|c]
                        1 & -2 & 1 & -1 & 1 & 0 \\
                        0 & 0 & 1 & -1 & 3 & -2 \\
                        0 & 0 & 0 & 1 & -2 & 1 \\
                        0 & 0 & 0 & 0 & 0 & a + 1
                    \end{bmatrix}$ &
                \end{tabularx}
                It follows that there is only a solution when $a = -1$. To clarify some definitions, we refer to a variable as being \textbf{basic}, if it corresponds to the pivot element of any given row, hence in this case, the basic variables are $x_1, x_3, x_4$, and the rest of the variables are \textbf{free} ($x_2$, and $x_5$ in our case). The pivot columns in our case (written as $\vec{p_i}$ corresponding to $x_i$) are;
                \begin{center}
                    $\vec{p_1} = \begin{bmatrix}
                        1 \\ 0 \\ 0 \\ 0
                    \end{bmatrix}$, $\vec{p_3} = \begin{bmatrix}
                        1 \\ 1 \\ 0 \\ 0
                    \end{bmatrix}$, and $\vec{p_4} = \begin{bmatrix}
                        -1 \\ -1 \\ 1 \\ 0
                    \end{bmatrix}$
                \end{center}
                We implicitly set the \textbf{free} variables to 0 when we find our particular solution using this method.
                \begin{center}
                    $x_1\vec{p_1} + x_3\vec{p_3} + x_4\vec{p_4} = \vec{b} \Leftrightarrow x_1\begin{bmatrix}
                        1 \\ 0 \\ 0 \\ 0
                    \end{bmatrix} + x_3\begin{bmatrix}
                        1 \\ 1 \\ 0 \\ 0
                    \end{bmatrix} + x_4\begin{bmatrix}
                        -1 \\ -1 \\ 1 \\ 0
                    \end{bmatrix} = \begin{bmatrix}
                        0 \\ -2 \\ 1 \\ 0
                    \end{bmatrix} \Leftrightarrow \vec{x} = \begin{bmatrix}
                        2 \\ 0 \\ -1 \\ 1 \\ 0
                    \end{bmatrix}$
                \end{center}
                We can get $x_4 = 1$ straight away, and from that we can get $x_3 = -1$, and also $x_1 = 2$. By using the same method as before, where we try create the non-pivot columns as a linear combination of the pivot columns, we can get the following general solution ($\forall \lambda_1, \lambda_2 \in \mathbb{R}$) - note that we could just as easily negate all the values in the column vectors, since our $\lambda_1, \lambda_2$ can take any real value;
                \begin{center}
                    $\vec{x} = \begin{bmatrix}
                        2 \\ 0 \\ -1 \\ 1 \\ 0
                    \end{bmatrix} + \lambda_1 \begin{bmatrix}
                        -2 \\ -1 \\ 0 \\ 0 \\ 0
                    \end{bmatrix} + \lambda_2 \begin{bmatrix}
                        -2 \\ 0 \\ 1 \\ -2 \\ -1
                    \end{bmatrix}$
                \end{center}
            \subsubsection*{Minus-1 Trick}
                We say that some equation system is in \textbf{reduced row echelon form (RREF)} (this may also be referred to as row-reduced echelon form, or row canonical form in other texts), if it is already in row echelon form, every pivot item is 1, and the pivot is the only non-zero entry in the column. Consider the following matrix, with the pivots coloured purple, and the matrix after adding rows of the form $\begin{bmatrix}
                    0 & \cdots & 0 & \textcolor{red}{-1} & 0 & \cdots & 0
                \end{bmatrix}$, to create a square matrix. By reading the columns containing the new \textcolor{red}{-1}s, we can get all solutions for $\mat{A}\vec{x} = \vec{0}$, once again $\forall \lambda_1, \lambda_2 \in \mathbb{R}$;
                \begin{center}
                    $\mat{A} = \begin{bmatrix}
                        \textcolor{violet}{1} & 3 & 0 & 0 & 3 \\
                        0 & 0 & \textcolor{violet}{1} & 0 & 9 \\
                        0 & 0 & 0 & \textcolor{violet}{1} & -4
                    \end{bmatrix}$ \hfill $\mat{A^\prime} = \begin{bmatrix}
                        \textcolor{violet}{1} & 3 & 0 & 0 & 3 \\
                        0 & \textcolor{red}{-1} & 0 & 0 & 0 \\
                        0 & 0 & \textcolor{violet}{1} & 0 & 9 \\
                        0 & 0 & 0 & \textcolor{violet}{1} & -4 \\
                        0 & 0 & 0 & 0 & \textcolor{red}{-1}
                    \end{bmatrix}$ \hfill $\vec{x} = \lambda_1 \begin{bmatrix}
                        3 \\ -1 \\ 0 \\ 0 \\ 0
                    \end{bmatrix} + \lambda_2 \begin{bmatrix}
                        3 \\ 0 \\ 9 \\ -4 \\ -1
                    \end{bmatrix}$
                \end{center}
            \subsubsection*{Gaussian Elimination to Calculate the Inverse}
                This is a fairly simple technique for calculating the inverse matrix, where we essentially augment the original matrix with the identity, and apply Gaussian elimination to get the identity on the left hand side; $\begin{bmatrix}[c|c]
                    \mat{A} & \mat{I}_n
                \end{bmatrix} \leadsto \cdots \leadsto \begin{bmatrix}[c|c]
                    \mat{I}_n & \mat{A}^{-1}
                \end{bmatrix}$.
                \medskip

                This is one of the parts in this module that are fairly easy to understand, but just requires practice for the exams. This technique is applied to a simple $\mat{A} \in \mathbb{R}^{2 \times 2}$;
                \begin{center}
                    $\begin{bmatrix}[cc|cc]
                        1 & 2 & 1 & 0 \\
                        3 & 4 & 0 & 1
                    \end{bmatrix} \begin{matrix}
                        {} \\ -3R_1
                    \end{matrix} \leadsto \begin{bmatrix}[cc|cc]
                        1 & 2 & 1 & 0 \\
                        0 & -2 & -3 & 1
                    \end{bmatrix} \begin{matrix}
                        {} \\ \cdot -\frac{1}{2}
                    \end{matrix} \leadsto \begin{bmatrix}[cc|cc]
                        1 & 2 & 1 & 0 \\
                        0 & 1 & \frac{3}{2} & -\frac{1}{2}
                    \end{bmatrix} \begin{matrix}
                        -2R_2 \\ {}
                    \end{matrix} \leadsto \begin{bmatrix}[cc|cc]
                        1 & 0 & -2 & 1 \\
                        0 & 1 & \frac{3}{2} & -\frac{1}{2}
                    \end{bmatrix}$
                \end{center}
            \subsubsection*{Vector Spaces}
                We define a real-valued \textbf{vector space} ($\mathbb{R}$-vector space) is a set $V$ with two operations;
                \begin{itemize}
                    \itemsep0em
                    \item $+ : V \times V \mapsto V$ \hfill inner operation
                    \item $\cdot : \mathbb{R} \times V \mapsto V$ \hfill outer operation
                \end{itemize}
                The following properties hold;
                \begin{itemize}
                    \itemsep0em
                    \item $(V, +)$ is an Abelian group
                    \item the outer operation is distributive
                        \subitem $\lambda \cdot (\vec{x} + \vec{y}) = \lambda \cdot \vec{x} + \lambda \cdot \vec{y}$ \hfill $\forall \lambda \in \mathbb{R}, \vec{x}, \vec{y} \in V$
                        \subitem $(\lambda + \psi) \cdot \vec{x} = \lambda \cdot \vec{x} + \psi \cdot \vec{x}$ \hfill $\forall \lambda, \psi \in \mathbb{R}, \vec{x} \in V$
                    \item the outer operation is associative
                        \subitem $\lambda \cdot (\psi \cdot \vec{x}) = (\lambda \cdot \psi) \cdot \vec{x}$ \hfill $\forall \lambda, \psi \in \mathbb{R}, \vec{x} \in V$
                    \item the outer operation has a neutral element of 1
                        \subitem $1 \cdot \vec{x} = \vec{x}$ \hfill $\forall \vec{x} \in V$
                \end{itemize}
                Examples of vector spaces are $V = \mathbb{R}^n$, $V = \mathbb{R}^{m \times n}$, and $V = \mathbb{C}$ (the standard representation of the complex numbers).
            \subsubsection*{Vector Subspaces}
                Suppose we have $U \subset V$, where $V$ is a vector space, then $U$ is a vector / linear subspace if it is a vector space with the same outer, and inner operations, but restricted to $U \times U$, and $\mathbb{R} \times U$, respectively.
                \medskip

                For us to show that $U$ is a vector subspace, we must show the following properties;
                \begin{itemize}
                    \itemsep0em
                    \item $U \neq \emptyset$, especially have to show $\vec{0} \in U$
                    \item closure of $U$
                        \subitem outer operation: $\lambda \vec{x} \in U$ \hfill $\forall \lambda \in \mathbb{R}, \vec{x} \in U$
                        \subitem inner operation: $\vec{x} + \vec{y} \in U$ \hfill $\forall \vec{x}, \vec{y} \in U$
                \end{itemize}
                We have trivial vector subspaces of the vector space $V$, namely $V$ itself, and $\{\vec{0}\}$ (the subspace containing only the zero vector). The solution space for a homogeneous linear equation system $\mat{A}\vec{x} = \vec{0}$, with $n$ unknowns, is a subspace of $\mathbb{R}^n$. Note that a homogeneous equation means that it has the zero vector on the right hand side. The same also applies in reverse; such that for a vector subspace $U \subset V$, there is a homogeneous system which characterises $U$. In contrast, an inhomogeneous linear equation system $\mat{A}\vec{x} = \vec{b}$, where $\vec{b} \neq \vec{0}$ does not have a vector subspace for its solutions. Assume that it does have some subspace $U$, therefore, $\vec{0} \in U$ by definition. However, $\vec{0}$ doesn't satisfy the equation, since we know $\vec{b} \neq \vec{0}$.
            \subsubsection*{Linear Independence}
\end{document}
